{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Custom Vector Database Operators for NVIDIA RAG\n",
    "\n",
    "\n",
    "## Overview\n",
    "\n",
    "This is an advanced notebook to demonstrate how to create and integrate custom vector database (VDB) operators with the NVIDIA RAG blueprint. This will guide you to build your own VDB implementations that work with `NvidiaRAG` and `NvidiaRAGIngestor` components.\n",
    "\n",
    "## Key Topics Covered\n",
    "\n",
    "- **VDB Operator Architecture** – Understanding the `VDBRag` base class and required interfaces\n",
    "- **Custom Implementation** – Building a complete [OpenSearch VDB](https://opensearch.org/) operator from scratch\n",
    "- **Integration Patterns** – Connecting your custom VDB with NVIDIA RAG pipelines\n",
    "- **Best Practices** – Patterns with error handling and lifecycle management\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- NVIDIA NGC API key for accessing models\n",
    "- Docker for running dependent services (Milvus, NIMs, etc.)\n",
    "- Python environment with NVIDIA RAG package installed\n",
    "- Basic understanding of vector databases and RAG concepts\n",
    "\n",
    "\n",
    "**Note:** This pattern can be adapted for any vector database by implementing the same interface methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup for NVIDIA RAG Python Package\n",
    "\n",
    "Please refer to [rag_library_usage.ipynb](./rag_library_usage.ipynb) for detailed installation instructions for rag libary.\n",
    "\n",
    "**Quick install (in a Python venv):**\n",
    "\n",
    "```bash\n",
    "# activate python venv using uv\n",
    "uv pip install nvidia-rag[all]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setting up the dependencies\n",
    "\n",
    "After the environment for the python package is set up, we launch all the dependent services and NIMs that the pipeline depends on.\n",
    "Fulfill the [prerequisites here](../docs/deploy-docker-self-hosted.md) to set up docker on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup the default configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install python-dotenv\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\".env_library\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your `NGC_API_KEY` after executing the cell below. You can obtain a key by following steps [here](../docs/api-key.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset if needed\n",
    "if os.environ.get(\"NGC_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NGC_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    candidate_api_key = getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert candidate_api_key.startswith(\"nvapi-\"), (\n",
    "        f\"{candidate_api_key[:5]}... is not a valid key\"\n",
    "    )\n",
    "    os.environ[\"NGC_API_KEY\"] = candidate_api_key\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = candidate_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to nvcr.io which is needed for pulling the containers of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup OpenSearch Vector DB using Docker Compose\n",
    "\n",
    "Follow these steps in cells below to set up OpenSearch as your vector database:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1. Create Docker Compose Configuration**\n",
    "\n",
    "Create a `docker-compose-opensearch.yaml` file in `deploy/compose/` directory by running the below cell.\n",
    "\n",
    "\n",
    "**[Optional] Advanced Configuration**\n",
    "- **Memory Settings**: Adjust `OPENSEARCH_JAVA_OPTS` based on your system resources\n",
    "- **Security**: Security plugin is disabled for simplicity. Enable for production use\n",
    "- **Network**: Uses `nvidia-rag` network for integration with other services\n",
    "- **Data Persistence**: To keep data peristent, the opensearch data can be mounted to external volume with following steps:\n",
    "  - Create volume directory and provide required permissions:\n",
    "    ```bash\n",
    "    sudo mkdir -p deploy/compose/volumes/opensearch/\n",
    "    sudo chmod -R 777 deploy/compose/volumes/opensearch/\n",
    "    ```\n",
    "  - Mount volume:\n",
    "    ```yaml\n",
    "        volumes:\n",
    "          - ./volumes/opensearch:/usr/share/opensearch/data/\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file_content = \"\"\"services:\n",
    "  opensearch:\n",
    "    image: opensearchproject/opensearch:3.1.0\n",
    "    ports:\n",
    "      - \"9200:9200\"\n",
    "      - \"9300:9300\"\n",
    "    environment:\n",
    "      - cluster.name=opensearch-cluster\n",
    "      - node.name=opensearch-node\n",
    "      - discovery.type=single-node\n",
    "      - bootstrap.memory_lock=true\n",
    "      - \"OPENSEARCH_JAVA_OPTS=-Xms512m -Xmx512m -XX:MaxDirectMemorySize=10g\"\n",
    "      - OPENSEARCH_INITIAL_ADMIN_PASSWORD=\"myStrongPassword123@456\"\n",
    "      - DISABLE_SECURITY_PLUGIN=true\n",
    "    healthcheck:\n",
    "      test: [ \"CMD\", \"curl\", \"-f\", \"http://localhost:9200\" ]\n",
    "      interval: 30s\n",
    "      timeout: 20s\n",
    "      retries: 3\n",
    "    profiles: [\"opensearch\"]\n",
    "\n",
    "networks:\n",
    "  default:\n",
    "    name: nvidia-rag\n",
    "\"\"\"\n",
    "\n",
    "with open(\"../deploy/compose/docker-compose-opensearch.yaml\", \"w\") as f:\n",
    "    f.write(yaml_file_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2. Start OpenSearch Service**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Start OpenSearch with the opensearch profile\n",
    "# Make sure elasticsearch container is not running (check with `docker ps`), since it uses same port\n",
    "# ! docker stop <elasticsearch_container_name> # Uncomment this if elasticsearch is running\n",
    "!docker compose -f ../deploy/compose/docker-compose-opensearch.yaml --profile opensearch up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4. Verify OpenSearch is Running and Healthy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if OpenSearch is Running\n",
    "!docker ps | grep opensearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if OpenSearch is healthy\n",
    "!curl -X GET \"localhost:9200/_cluster/health?pretty\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5: Install OpenSearch Python Client**\n",
    "\n",
    "Install the `opensearch-py` client in your current environment. This allows your Python code to connect to and interact with the OpenSearch service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install OpenSearch Python Client in your environment\n",
    "!uv pip install opensearch-py # Use `pip install opensearch-py` if you are not using uv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup other dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure MinIO is running (required for Citations)\n",
    "!docker compose -f ../deploy/compose/vectordb.yaml --profile minio up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setup the NIMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 1: Deploy on-prem models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to Option 2 if you are interested in using NVIDIA-hosted models.\n",
    "\n",
    "Ensure you meet [the hardware requirements](../docs/support-matrix.md). By default the NIMs are configured to use 2xH100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model cache directory\n",
    "!mkdir -p ~/.cache/model-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MODEL_DIRECTORY environment variable in the Python kernel\n",
    "import os\n",
    "\n",
    "os.environ[\"MODEL_DIRECTORY\"] = os.path.expanduser(\"~/.cache/model-cache\")\n",
    "print(\"MODEL_DIRECTORY set to:\", os.environ[\"MODEL_DIRECTORY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GPU IDs for the various microservices if needed\n",
    "os.environ[\"EMBEDDING_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"RANKING_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_GRAPHICS_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_TABLE_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"OCR_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"LLM_MS_GPU_ID\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select your hardware-specific profile name as per the guidance provided in [NIM Model Profile Configuration](../docs/model-profiles.md) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NIM_MODEL_PROFILE\"] = \"......\" # Populate your profile name as per hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploying NIMs - This may take a while as models download. If kernel times out, just rerun this cell.\n",
    "!USERID=$(id -u) docker compose -f ../deploy/compose/nims.yaml up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the status of running containers (run this cell repeatedly or in a terminal)\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure all the below are running and healthy before proceeding further\n",
    "```output\n",
    "NAMES                           STATUS\n",
    "nemoretriever-ranking-ms        Up ... (healthy)\n",
    "compose-page-elements-1         Up ...\n",
    "compose-paddle-1                Up ...\n",
    "compose-graphic-elements-1      Up ...\n",
    "compose-table-structure-1       Up ...\n",
    "nemoretriever-embedding-ms      Up ... (healthy)\n",
    "nim-llm-ms                      Up ... (healthy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Using Nvidia Hosted models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"APP_LLM_MODELNAME\"] = \"nvidia/llama-3.3-nemotron-super-49b-v1.5\"\n",
    "os.environ[\"APP_EMBEDDINGS_MODELNAME\"] = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
    "os.environ[\"APP_RANKING_MODELNAME\"] = \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n",
    "os.environ[\"APP_EMBEDDINGS_SERVERURL\"] = \"\"\n",
    "os.environ[\"APP_LLM_SERVERURL\"] = \"\"\n",
    "os.environ[\"APP_RANKING_SERVERURL\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/retrieval/nvidia/llama-3_2-nv-rerankqa-1b-v2/reranking/v1\"\n",
    ")\n",
    "os.environ[\"OCR_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/baidu/paddleocr\"\n",
    "os.environ[\"OCR_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2\"\n",
    ")\n",
    "os.environ[\"YOLOX_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1\"\n",
    ")\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1\"\n",
    ")\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL\"] = \"http\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setup the Nvidia Ingest runtime and redis service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!docker compose -f ../deploy/compose/docker-compose-ingestor-server.yaml up nv-ingest-ms-runtime redis -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Custom VDB Operator - Library Mode\n",
    "\n",
    "Create a custom Vector Database operator by implementing the `VDBRag` interface, then pass it to `NvidiaRAG`/`NvidiaRAGIngestor`.\n",
    "\n",
    "Refer to [`docs/change-vectordb.md`](../docs/change-vectordb.md#integrate-in-library-mode-developer-friendly) for detailed steps to integrate your own vector database with Nvidia RAG in library mode.\n",
    "\n",
    "**Note:** \n",
    "The implementation below demonstrates integration in library mode.\n",
    "If you wish to proceed with server mode, please refer to the documentation here: [Integrate Into NVIDIA RAG (Server Mode)](../docs/change-vectordb.md#integrate-into-nvidia-rag-server-mode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Defining your own VDBRag class for Opensearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining Helper functions for OpenSearch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper functions for OpenSearch query construction.\n",
    "\n",
    "This cell provides utility functions to generate OpenSearch queries for:\n",
    "- Retrieving all unique document sources (`get_unique_sources_query`)\n",
    "- Deleting a metadata schema by collection name (`get_delete_metadata_schema_query`)\n",
    "- Fetching the metadata schema for a specific collection (`get_metadata_schema_query`)\n",
    "\n",
    "These helpers are used by the custom VDB operator to manage and query metadata in OpenSearch.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def get_unique_sources_query():\n",
    "    \"\"\"\n",
    "    Generate aggregation query to retrieve all unique document sources.\n",
    "    \"\"\"\n",
    "    query_unique_sources = {\n",
    "        \"size\": 0,\n",
    "        \"aggs\": {\n",
    "            \"unique_sources\": {\n",
    "                \"composite\": {\n",
    "                    \"size\": 1000,  # Adjust size depending on number of unique values\n",
    "                    \"sources\": [\n",
    "                        {\n",
    "                            \"source_name\": {\n",
    "                                \"terms\": {\n",
    "                                    \"field\": \"metadata.source.source_name.keyword\"\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    ],\n",
    "                },\n",
    "                \"aggs\": {\n",
    "                    \"top_hit\": {\n",
    "                        \"top_hits\": {\n",
    "                            \"size\": 1  # Just one document per source_name\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    return query_unique_sources\n",
    "\n",
    "\n",
    "def get_delete_metadata_schema_query(collection_name: str):\n",
    "    \"\"\"\n",
    "    Create deletion query for removing metadata schema by collection name.\n",
    "    \"\"\"\n",
    "    query_delete_metadata_schema = {\n",
    "        \"query\": {\"term\": {\"collection_name.keyword\": collection_name}}\n",
    "    }\n",
    "    return query_delete_metadata_schema\n",
    "\n",
    "\n",
    "def get_metadata_schema_query(collection_name: str):\n",
    "    \"\"\"\n",
    "    Build search query to retrieve metadata schema for specified collection.\n",
    "    \"\"\"\n",
    "    query_metadata_schema = {\"query\": {\"term\": {\"collection_name\": collection_name}}}\n",
    "    return query_metadata_schema\n",
    "\n",
    "\n",
    "def get_delete_docs_query(source_value: str):\n",
    "    \"\"\"\n",
    "    Construct deletion query for documents matching the source value.\n",
    "    \"\"\"\n",
    "    query_delete_documents = {\n",
    "        \"query\": {\"term\": {\"metadata.source.source_name.keyword\": source_value}}\n",
    "    }\n",
    "    return query_delete_documents\n",
    "\n",
    "\n",
    "def create_metadata_collection_mapping():\n",
    "    \"\"\"Generate Elasticsearch index mapping for metadata schema collections.\"\"\"\n",
    "    return {\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"collection_name\": {\n",
    "                    \"type\": \"keyword\"  # or \"text\" depending on your search needs\n",
    "                },\n",
    "                \"metadata_schema\": {\n",
    "                    \"type\": \"object\",  # For JSON-like structure\n",
    "                    \"enabled\": True,  # Set to False if you don't want to index its fields\n",
    "                },\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the Opensearch `VDBRag` class**\n",
    "\n",
    "The following cell contains the implementation of each method of the `VDBRag` class. For a comprehensive understanding of the functionality and design of each method, it is highly recommended to carefully review the respective docstrings provided with each implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from typing import Any\n",
    "\n",
    "from nvidia_rag.utils.vdb.vdb_base import VDBRag\n",
    "from nvidia_rag.utils.vdb import DEFAULT_METADATA_SCHEMA_COLLECTION\n",
    "\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.runnables import RunnableAssign, RunnableLambda\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "\n",
    "from nv_ingest_client.util.milvus import cleanup_records\n",
    "from opentelemetry import context as otel_context\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class OpenSearchVDB(VDBRag):\n",
    "    \"\"\"\n",
    "    OpenSearchVDB is a vector database implementation using OpenSearch for RAG (Retrieval-Augmented Generation) applications.\n",
    "\n",
    "    This class provides comprehensive functionality for document ingestion, indexing, retrieval, and metadata management\n",
    "    using OpenSearch as the backend vector database. It supports embedding-based similarity search, metadata filtering,\n",
    "    and collection management operations.\n",
    "\n",
    "    Inherits from VDBRag to provide a standardized interface for vector database operations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        opensearch_url=\"http://localhost:9200\",\n",
    "        index_name=\"test\",\n",
    "        meta_dataframe=None,\n",
    "        meta_source_field=None,\n",
    "        meta_fields=None,\n",
    "        embedding_model=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the OpenSearchVDB instance with connection parameters and metadata configuration.\n",
    "\n",
    "        Args:\n",
    "            opensearch_url (str, optional): The URL endpoint for the OpenSearch cluster.\n",
    "                                          Defaults to \"http://localhost:9200\".\n",
    "            index_name (str, optional): The name of the OpenSearch index to use for storing documents.\n",
    "                                      Defaults to \"test\".\n",
    "            meta_dataframe (pandas.DataFrame, optional): DataFrame containing metadata information for documents.\n",
    "                                                       Used for enriching document metadata during ingestion.\n",
    "            meta_source_field (str, optional): The field name in meta_dataframe that corresponds to document sources.\n",
    "                                             Used for joining metadata with documents.\n",
    "            meta_fields (list, optional): List of metadata field names to extract and store with documents.\n",
    "                                        These fields will be searchable and filterable.\n",
    "            embedding_model (object, optional): The embedding model instance used for generating vector embeddings.\n",
    "                                              Must be compatible with langchain embedding interfaces.\n",
    "\n",
    "        Returns:\n",
    "            None: This is a constructor method that initializes the instance.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the specified OpenSearch URL.\n",
    "            ValueError: If invalid parameters are provided for index_name or embedding_model.\n",
    "\n",
    "        Example:\n",
    "            >>> from sentence_transformers import SentenceTransformer\n",
    "            >>> embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            >>> vdb = OpenSearchVDB(\n",
    "            ...     opensearch_url=\"http://localhost:9200\",\n",
    "            ...     index_name=\"my_documents\",\n",
    "            ...     embedding_model=embedding_model\n",
    "            ... )\n",
    "        \"\"\"\n",
    "        self.opensearch_url = opensearch_url\n",
    "        self.embedding_model = embedding_model\n",
    "        self.index_name = index_name\n",
    "        self.opensearch_vs = self.get_langchain_vectorstore(\n",
    "            collection_name=index_name\n",
    "        )\n",
    "        self.meta_dataframe = meta_dataframe\n",
    "        self.meta_source_field = meta_source_field\n",
    "        self.meta_fields = meta_fields\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    @property\n",
    "    def collection_name(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the current collection name (index name) for the OpenSearch vector database.\n",
    "\n",
    "        This property provides a standardized interface to access the collection name,\n",
    "        which is internally stored as index_name in OpenSearch terminology.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "\n",
    "        Returns:\n",
    "            str: The current collection/index name being used for document storage and retrieval.\n",
    "\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(index_name=\"my_collection\")\n",
    "            >>> print(vdb.collection_name)\n",
    "            'my_collection'\n",
    "        \"\"\"\n",
    "        return self.index_name\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    @collection_name.setter\n",
    "    def collection_name(self, collection_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Set the collection name (index name) for the OpenSearch vector database.\n",
    "\n",
    "        This property setter allows changing the target collection/index for subsequent\n",
    "        operations. The change affects all future document storage and retrieval operations.\n",
    "\n",
    "        Args:\n",
    "            collection_name (str): The new collection/index name to use. Must be a valid\n",
    "                                 OpenSearch index name (lowercase, no spaces, valid characters).\n",
    "\n",
    "        Returns:\n",
    "            None: This is a setter method that modifies the instance state.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If the collection_name contains invalid characters for OpenSearch index names.\n",
    "\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB()\n",
    "            >>> vdb.collection_name = \"new_collection\"\n",
    "            >>> print(vdb.collection_name)\n",
    "            'new_collection'\n",
    "        \"\"\"\n",
    "        self.index_name = collection_name\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def create_index(\n",
    "        self\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Create a new OpenSearch index if it doesn't already exist.\n",
    "\n",
    "        This method initializes a new OpenSearch index with predefined settings optimized\n",
    "        for vector similarity search using FAISS engine. The index is created with 2048\n",
    "        dimensions to accommodate typical embedding models.\n",
    "\n",
    "        Args:\n",
    "            None (uses instance attributes)\n",
    "\n",
    "        Returns:\n",
    "            None: This method performs index creation as a side effect.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            PermissionError: If insufficient permissions to create indices on the cluster.\n",
    "            ValueError: If the index_name contains invalid characters.\n",
    "\n",
    "        Side Effects:\n",
    "            - Creates a new OpenSearch index if it doesn't exist\n",
    "            - Configures the index with FAISS engine for vector search\n",
    "            - Sets dimension to 2048 for vector embeddings\n",
    "\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(index_name=\"my_new_index\")\n",
    "            >>> vdb.create_index()  # Creates index if not exists\n",
    "        \"\"\"\n",
    "        if not self.check_collection_exists(self.index_name):\n",
    "            self.opensearch_vs.create_index(\n",
    "                index_name=self.index_name,\n",
    "                dimension=2048,\n",
    "                engine=\"faiss\"\n",
    "            )\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def write_to_index(self, records: list, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Write processed records with embeddings to the OpenSearch index.\n",
    "\n",
    "        This method processes raw records by cleaning them, extracting text and vector embeddings,\n",
    "        organizing metadata, and then storing everything in the OpenSearch index. The records are\n",
    "        preprocessed using the cleanup_records utility to ensure consistent formatting.\n",
    "\n",
    "        Args:\n",
    "            records (list): List of record dictionaries containing document data. Each record should have:\n",
    "                          - 'text': The document text content\n",
    "                          - 'vector': Pre-computed embedding vector (list/array of floats)\n",
    "                          - 'source': Source identifier for the document\n",
    "                          - 'content_metadata': Additional metadata as dictionary\n",
    "            **kwargs: Additional keyword arguments (currently unused but available for extension)\n",
    "\n",
    "        Returns:\n",
    "            None: This method performs bulk indexing as a side effect.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If records are malformed or missing required fields.\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            IndexError: If the target index doesn't exist or is misconfigured.\n",
    "\n",
    "        Side Effects:\n",
    "            - Processes and cleans input records using configured metadata settings\n",
    "            - Bulk inserts documents with embeddings into OpenSearch index\n",
    "            - Refreshes the index to make documents immediately searchable\n",
    "            - Logs the number of successfully added records\n",
    "\n",
    "        Example:\n",
    "            >>> records = [\n",
    "            ...     {\n",
    "            ...         'text': 'Sample document text',\n",
    "            ...         'vector': [0.1, 0.2, ...],  # 2048-dim embedding\n",
    "            ...         'source': 'doc1.pdf',\n",
    "            ...         'content_metadata': {'page': 1, 'section': 'intro'}\n",
    "            ...     }\n",
    "            ... ]\n",
    "            >>> vdb.write_to_index(records)\n",
    "        \"\"\"\n",
    "        # Clean up and flatten records to pull appropriate fields from the records\n",
    "        cleaned_records = cleanup_records(\n",
    "            records=records,\n",
    "            meta_dataframe=self.meta_dataframe,\n",
    "            meta_source_field=self.meta_source_field,\n",
    "            meta_fields=self.meta_fields,\n",
    "        )\n",
    "\n",
    "        # Prepare texts, embeddings, and metadatas from cleaned records\n",
    "        texts, embeddings, metadatas = [], [], []\n",
    "        for cleaned_record in cleaned_records:\n",
    "            texts.append(cleaned_record.get(\"text\"))\n",
    "            embeddings.append(cleaned_record.get(\"vector\"))\n",
    "            metadatas.append(\n",
    "                {\n",
    "                    \"source\": cleaned_record.get(\"source\"),\n",
    "                    \"content_metadata\": cleaned_record.get(\"content_metadata\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Add texts, embeddings, and metadatas to the OpenSearch index\n",
    "        self.opensearch_vs.add_embeddings(\n",
    "            text_embeddings=zip(texts, embeddings),\n",
    "            metadatas=metadatas,\n",
    "        )\n",
    "        logger.info(\n",
    "            f\"Added {len(texts)} records to Opensearch index {self.index_name}\"\n",
    "        )\n",
    "        self.opensearch_vs.client.indices.refresh(index=self.index_name)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def retrieval(self, queries: list, **kwargs) -> list[dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents from OpenSearch based on input queries.\n",
    "\n",
    "        This method performs semantic similarity search using vector embeddings to find\n",
    "        the most relevant documents for the given queries. Currently, this is a placeholder\n",
    "        method that requires implementation for the specific OpenSearchVDB use case.\n",
    "\n",
    "        Args:\n",
    "            queries (list): List of query strings to search for. Each query will be processed\n",
    "                          to find the most semantically similar documents in the index.\n",
    "            **kwargs: Additional keyword arguments for retrieval configuration such as:\n",
    "                    - top_k (int): Number of top results to return per query\n",
    "                    - filter_expr (dict): Metadata filters to apply during search\n",
    "                    - threshold (float): Minimum similarity score threshold\n",
    "\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: List of retrieved documents with their metadata and scores.\n",
    "                                Each document dict contains:\n",
    "                                - 'text': The document content\n",
    "                                - 'metadata': Document metadata including source info\n",
    "                                - 'score': Similarity score for the query\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: This method is currently not implemented and serves as a placeholder.\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            ValueError: If queries are malformed or empty.\n",
    "\n",
    "        Example:\n",
    "            >>> queries = [\"What is machine learning?\", \"How does AI work?\"]\n",
    "            >>> results = vdb.retrieval(queries, top_k=5)\n",
    "            # Currently raises NotImplementedError\n",
    "        \"\"\"\n",
    "        # Placeholder: implement actual retrieval logic\n",
    "        raise NotImplementedError(\"retrieval must be implemented for OpenSearchVDB\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def reindex(self, records: list, **kwargs) -> None:\n",
    "        \"\"\"\n",
    "        Reindex existing documents in the OpenSearch index with updated data or embeddings.\n",
    "\n",
    "        This method handles the reindexing of documents that already exist in the OpenSearch\n",
    "        index. It can be used to update document content, refresh embeddings, or modify\n",
    "        metadata without losing the original document structure.\n",
    "\n",
    "        Args:\n",
    "            records (list): List of record dictionaries containing updated document data.\n",
    "                          Each record should include identifiers to match existing documents.\n",
    "            **kwargs: Additional keyword arguments for reindexing configuration such as:\n",
    "                    - batch_size (int): Number of documents to process in each batch\n",
    "                    - update_embeddings (bool): Whether to regenerate embeddings\n",
    "                    - preserve_metadata (bool): Whether to keep existing metadata\n",
    "\n",
    "        Returns:\n",
    "            None: This method performs reindexing as a side effect.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: This method is currently not implemented and serves as a placeholder.\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            ValueError: If records are malformed or missing required identifiers.\n",
    "            IndexError: If target documents don't exist in the index.\n",
    "\n",
    "        Side Effects:\n",
    "            - Updates existing documents in the OpenSearch index\n",
    "            - Refreshes embeddings if specified\n",
    "            - Maintains document version history for conflict resolution\n",
    "\n",
    "        Example:\n",
    "            >>> updated_records = [\n",
    "            ...     {\n",
    "            ...         'id': 'doc_123',\n",
    "            ...         'text': 'Updated document content',\n",
    "            ...         'vector': [0.2, 0.3, ...],  # New embedding\n",
    "            ...         'metadata': {'version': 2}\n",
    "            ...     }\n",
    "            ... ]\n",
    "            >>> vdb.reindex(updated_records)\n",
    "            # Currently raises NotImplementedError\n",
    "        \"\"\"\n",
    "        # Placeholder: implement actual reindex logic\n",
    "        raise NotImplementedError(\"reindex must be implemented for OpenSearchVDB\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def run(\n",
    "        self,\n",
    "        records: list,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Execute the complete document ingestion pipeline for OpenSearch.\n",
    "\n",
    "        This is the main orchestration method that performs the full workflow of\n",
    "        document ingestion: creating the index if it doesn't exist, then writing\n",
    "        all provided records to the index. This method provides a simple interface\n",
    "        for bulk document processing.\n",
    "\n",
    "        Args:\n",
    "            records (list): List of record dictionaries containing document data to ingest.\n",
    "                          Each record should follow the format expected by write_to_index().\n",
    "\n",
    "        Returns:\n",
    "            None: This method performs the complete ingestion workflow as a side effect.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            ValueError: If records are malformed or missing required fields.\n",
    "            PermissionError: If insufficient permissions for index creation or document insertion.\n",
    "\n",
    "        Side Effects:\n",
    "            - Creates the target index if it doesn't already exist\n",
    "            - Bulk inserts all provided records into the OpenSearch index\n",
    "            - Refreshes the index to make documents immediately searchable\n",
    "            - Logs the ingestion progress and results\n",
    "\n",
    "        Example:\n",
    "            >>> records = [\n",
    "            ...     {'text': 'Doc 1 content', 'vector': [...], 'source': 'doc1.pdf'},\n",
    "            ...     {'text': 'Doc 2 content', 'vector': [...], 'source': 'doc2.pdf'}\n",
    "            ... ]\n",
    "            >>> vdb.run(records)  # Complete ingestion workflow\n",
    "        \"\"\"\n",
    "        self.create_index()\n",
    "        self.write_to_index(records)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Methods for the VDBRag class for ingestion\n",
    "    async def check_health(self) -> dict[str, Any]:\n",
    "        \"\"\"Check Opensearch database health\"\"\"\n",
    "        status = {\n",
    "            \"service\": \"Opensearch\",\n",
    "            \"url\": self.opensearch_url,\n",
    "            \"status\": \"unknown\",\n",
    "            \"error\": None,\n",
    "        }\n",
    "\n",
    "        if not self.opensearch_url:\n",
    "            status[\"status\"] = \"skipped\"\n",
    "            status[\"error\"] = \"No URL provided\"\n",
    "            return status\n",
    "\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "\n",
    "            cluster_health = self.opensearch_vs.client.cluster.health()\n",
    "            indices = self.opensearch_vs.client.cat.indices(format=\"json\")\n",
    "\n",
    "            status[\"status\"] = \"healthy\"\n",
    "            status[\"latency_ms\"] = round((time.time() - start_time) * 1000, 2)\n",
    "            status[\"indices\"] = len(indices)\n",
    "            status[\"cluster_status\"] = cluster_health.get(\"status\", \"unknown\")\n",
    "\n",
    "        except ImportError:\n",
    "            status[\"status\"] = \"error\"\n",
    "            status[\"error\"] = (\n",
    "                \"Opensearch client not available (opensearch-py library not installed)\"\n",
    "            )\n",
    "        except Exception as e:\n",
    "            status[\"status\"] = \"error\"\n",
    "            status[\"error\"] = str(e)\n",
    "\n",
    "        return status\n",
    "\n",
    "    def create_collection(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "        dimension: int = 2048,\n",
    "        collection_type: str = \"text\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Create a new collection (index) in OpenSearch with specified configuration.\n",
    "\n",
    "        This method implements the VDBRag interface for collection creation, providing\n",
    "        a standardized way to create new document collections with appropriate vector\n",
    "        search configuration.\n",
    "\n",
    "        Args:\n",
    "            collection_name (str): Name of the collection to create. Must be a valid\n",
    "                                 OpenSearch index name (lowercase, no spaces).\n",
    "            dimension (int, optional): Dimensionality of the vector embeddings to store.\n",
    "                                     Defaults to 2048 to match common embedding models.\n",
    "            collection_type (str, optional): Type of collection being created.\n",
    "                                           Defaults to \"text\" for text document storage.\n",
    "\n",
    "        Returns:\n",
    "            None: This method creates the collection as a side effect.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            PermissionError: If insufficient permissions to create indices.\n",
    "            ValueError: If collection_name contains invalid characters.\n",
    "\n",
    "        Side Effects:\n",
    "            - Creates a new OpenSearch index with vector search capabilities\n",
    "            - Configures the index for the specified embedding dimensions\n",
    "            - Sets up FAISS engine for efficient similarity search\n",
    "\n",
    "        Example:\n",
    "            >>> vdb.create_collection(\"my_documents\", dimension=768, collection_type=\"text\")\n",
    "        \"\"\"\n",
    "        self.create_index()\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def check_collection_exists(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "    ) -> bool:\n",
    "        \"\"\"\n",
    "        Check whether a specified collection (index) exists in OpenSearch.\n",
    "\n",
    "        This method provides a simple boolean check to determine if a collection\n",
    "        is already present in the OpenSearch cluster before attempting operations\n",
    "        that require the collection to exist.\n",
    "\n",
    "        Args:\n",
    "            collection_name (str): Name of the collection to check for existence.\n",
    "                                 Should be a valid OpenSearch index name.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the collection exists, False otherwise.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            PermissionError: If insufficient permissions to check index existence.\n",
    "\n",
    "        Example:\n",
    "            >>> exists = vdb.check_collection_exists(\"my_documents\")\n",
    "            >>> if exists:\n",
    "            ...     print(\"Collection already exists\")\n",
    "            >>> else:\n",
    "            ...     vdb.create_collection(\"my_documents\")\n",
    "        \"\"\"\n",
    "        return self.opensearch_vs.client.indices.exists(index=collection_name)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def get_collection(self) -> list[dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve comprehensive information about all collections in the OpenSearch cluster.\n",
    "\n",
    "        This method scans the OpenSearch cluster for all available indices (collections),\n",
    "        retrieves their document counts, and fetches associated metadata schemas to provide\n",
    "        a complete overview of the vector database contents.\n",
    "\n",
    "        Args:\n",
    "            None (operates on the entire OpenSearch cluster)\n",
    "\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: List of collection information dictionaries. Each dict contains:\n",
    "                                - 'collection_name': Name of the collection/index\n",
    "                                - 'num_entities': Number of documents in the collection\n",
    "                                - 'metadata_schema': Schema definition for metadata fields\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            PermissionError: If insufficient permissions to list indices or access metadata.\n",
    "\n",
    "        Side Effects:\n",
    "            - Creates metadata schema collection if it doesn't exist\n",
    "            - Queries all non-hidden indices in the cluster\n",
    "            - Retrieves metadata schemas for each collection\n",
    "\n",
    "        Example:\n",
    "            >>> collections = vdb.get_collection()\n",
    "            >>> for collection in collections:\n",
    "            ...     print(f\"Collection: {collection['collection_name']}\")\n",
    "            ...     print(f\"Documents: {collection['num_entities']}\")\n",
    "            ...     print(f\"Schema: {collection['metadata_schema']}\")\n",
    "        \"\"\"\n",
    "        self.create_metadata_schema_collection()\n",
    "        indices = self.opensearch_vs.client.cat.indices(format=\"json\")\n",
    "        collection_info = []\n",
    "        for index in indices:\n",
    "            index_name = index[\"index\"]\n",
    "            if not index_name.startswith(\".\"):  # Ignore hidden indices\n",
    "                metadata_schema = self.get_metadata_schema(index_name)\n",
    "                collection_info.append(\n",
    "                    {\n",
    "                        \"collection_name\": index_name,\n",
    "                        \"num_entities\": index[\"docs.count\"],\n",
    "                        \"metadata_schema\": metadata_schema,\n",
    "                    }\n",
    "                )\n",
    "        return collection_info\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def delete_collections(\n",
    "        self,\n",
    "        collection_names: list[str],\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Delete multiple collections (indices) and their associated metadata from OpenSearch.\n",
    "\n",
    "        This method performs a comprehensive deletion of collections, removing both the\n",
    "        main document indices and their associated metadata schemas. The deletion is\n",
    "        performed safely with ignore_unavailable=True to handle missing collections gracefully.\n",
    "\n",
    "        Args:\n",
    "            collection_names (list[str]): List of collection names to delete. Each name\n",
    "                                        should be a valid OpenSearch index name.\n",
    "\n",
    "        Returns:\n",
    "            dict: Deletion summary containing:\n",
    "                - 'message': Status message about the deletion process\n",
    "                - 'successful': List of successfully deleted collection names\n",
    "                - 'failed': List of collection names that failed to delete\n",
    "                - 'total_success': Count of successful deletions\n",
    "                - 'total_failed': Count of failed deletions\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to OpenSearch cluster.\n",
    "            PermissionError: If insufficient permissions to delete indices.\n",
    "\n",
    "        Side Effects:\n",
    "            - Deletes the specified collections/indices from OpenSearch\n",
    "            - Removes associated metadata schemas from the metadata collection\n",
    "            - Logs the deletion results for auditing purposes\n",
    "            - Ignores collections that don't exist (fail-safe operation)\n",
    "\n",
    "        Example:\n",
    "            >>> result = vdb.delete_collections([\"old_docs\", \"temp_collection\"])\n",
    "            >>> print(f\"Deleted: {result['successful']}\")\n",
    "            >>> print(f\"Failed: {result['failed']}\")\n",
    "        \"\"\"\n",
    "        _ = self.opensearch_vs.client.indices.delete(\n",
    "            index=\",\".join(collection_names), ignore_unavailable=True\n",
    "        )\n",
    "        deleted_collections, failed_collections = collection_names, []\n",
    "        logger.info(f\"Collections deleted: {deleted_collections}\")\n",
    "\n",
    "        # Delete the metadata schema from the collection\n",
    "        for collection_name in deleted_collections:\n",
    "            _ = self.opensearch_vs.client.delete_by_query(\n",
    "                index=DEFAULT_METADATA_SCHEMA_COLLECTION,\n",
    "                body=get_delete_metadata_schema_query(collection_name),\n",
    "            )\n",
    "        return {\n",
    "            \"message\": \"Collection deletion process completed.\",\n",
    "            \"successful\": deleted_collections,\n",
    "            \"failed\": failed_collections,\n",
    "            \"total_success\": len(deleted_collections),\n",
    "            \"total_failed\": len(failed_collections),\n",
    "        }\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def get_documents(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Retrieve all unique documents from the specified collection.\n",
    "\n",
    "        This method queries the OpenSearch index to find all unique documents based on their\n",
    "        source names. It aggregates documents by their source and returns metadata information\n",
    "        for each unique document.\n",
    "\n",
    "        Args:\n",
    "            collection_name (str): The name of the collection/index to retrieve documents from.\n",
    "\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: A list of dictionaries containing document information.\n",
    "                Each dictionary has the following structure:\n",
    "                - document_name (str): The basename of the source file\n",
    "                - metadata (dict): Dictionary containing metadata fields and their values\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If the collection_name is invalid or doesn't exist.\n",
    "            IndexError: If the aggregation query fails or returns unexpected results.\n",
    "\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(index_name=\"my_collection\")\n",
    "            >>> documents = vdb.get_documents(\"my_collection\")\n",
    "            >>> print(documents[0])\n",
    "            {\n",
    "                'document_name': 'example.pdf',\n",
    "                'metadata': {'author': 'John Doe', 'created': '2024-01-01'}\n",
    "            }\n",
    "        \"\"\"\n",
    "        # metadata_schema = self.get_metadata_schema(collection_name)\n",
    "        metadata_schema = []\n",
    "        response = self.opensearch_vs.client.search(\n",
    "            index=collection_name,\n",
    "            body=get_unique_sources_query(),\n",
    "        )\n",
    "        documents_list = []\n",
    "        for hit in response[\"aggregations\"][\"unique_sources\"][\"buckets\"]:\n",
    "            source_name = hit[\"key\"][\"source_name\"]\n",
    "            metadata = (\n",
    "                hit[\"top_hit\"][\"hits\"][\"hits\"][0][\"_source\"]\n",
    "                .get(\"metadata\", {})\n",
    "                .get(\"content_metadata\", {})\n",
    "            )\n",
    "            metadata_dict = {}\n",
    "            for metadata_item in metadata_schema:\n",
    "                metadata_name = metadata_item.get(\"name\")\n",
    "                metadata_value = metadata.get(metadata_name, None)\n",
    "                metadata_dict[metadata_name] = metadata_value\n",
    "            documents_list.append(\n",
    "                {\n",
    "                    \"document_name\": os.path.basename(source_name),\n",
    "                    \"metadata\": metadata_dict,\n",
    "                }\n",
    "            )\n",
    "        return documents_list\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def delete_documents(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "        source_values: list[str],\n",
    "    ) -> bool:\n",
    "        \"\"\"Delete documents from a collection by source values.\n",
    "\n",
    "        This method removes documents from the OpenSearch index that match the provided\n",
    "        source values. It performs bulk deletion and refreshes the index to ensure\n",
    "        changes are immediately visible.\n",
    "\n",
    "        Args:\n",
    "            collection_name (str): The name of the collection/index to delete documents from.\n",
    "            source_values (list[str]): List of source identifiers for documents to delete.\n",
    "                These should match the 'source' field values in the stored documents.\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the deletion operation completes successfully, False otherwise.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If collection_name is invalid or source_values is empty.\n",
    "            PermissionError: If insufficient permissions to delete documents from the index.\n",
    "\n",
    "        Side Effects:\n",
    "            - Removes matching documents from the OpenSearch index\n",
    "            - Refreshes the index to make deletions immediately visible\n",
    "            - Logs the deletion operations for audit purposes\n",
    "\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(index_name=\"my_collection\")\n",
    "            >>> sources_to_delete = [\"document1.pdf\", \"document2.pdf\"]\n",
    "            >>> success = vdb.delete_documents(\"my_collection\", sources_to_delete)\n",
    "            >>> print(f\"Deletion successful: {success}\")\n",
    "            True\n",
    "        \"\"\"\n",
    "        for source_value in source_values:\n",
    "            self.opensearch_vs.client.delete_by_query(\n",
    "                index=collection_name, body=get_delete_docs_query(source_value)\n",
    "            )\n",
    "        self.opensearch_vs.client.indices.refresh(index=collection_name)\n",
    "        return True\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def create_metadata_schema_collection(\n",
    "        self,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the metadata schema storage collection.\n",
    "\n",
    "        This method creates a dedicated OpenSearch index for storing metadata schemas\n",
    "        associated with document collections. The schema collection uses a predefined\n",
    "        mapping optimized for metadata field definitions and collection associations.\n",
    "\n",
    "        Args:\n",
    "            None (uses instance attributes for configuration)\n",
    "\n",
    "        Returns:\n",
    "            None: This method performs index creation as a side effect.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            PermissionError: If insufficient permissions to create indices.\n",
    "            ValueError: If the default metadata schema collection name is invalid.\n",
    "\n",
    "        Side Effects:\n",
    "            - Creates the DEFAULT_METADATA_SCHEMA_COLLECTION index if it doesn't exist\n",
    "            - Applies the metadata collection mapping for schema storage\n",
    "            - Logs the creation status and configuration details\n",
    "\n",
    "        Note:\n",
    "            This is typically called once during system initialization. Subsequent calls\n",
    "            will detect the existing collection and log that it already exists.\n",
    "\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(opensearch_url=\"http://localhost:9200\")\n",
    "            >>> vdb.create_metadata_schema_collection()\n",
    "            # Creates the metadata schema storage index\n",
    "        \"\"\"\n",
    "        mapping = create_metadata_collection_mapping()\n",
    "        if not self.check_collection_exists(collection_name=DEFAULT_METADATA_SCHEMA_COLLECTION):\n",
    "            self.opensearch_vs.client.indices.create(\n",
    "                index=DEFAULT_METADATA_SCHEMA_COLLECTION, body=mapping\n",
    "            )\n",
    "            logging_message = (\n",
    "                f\"Collection {DEFAULT_METADATA_SCHEMA_COLLECTION} created \"\n",
    "                + f\"at {self.opensearch_url} with mapping {mapping}\"\n",
    "            )\n",
    "            logger.info(logging_message)\n",
    "        else:\n",
    "            logging_message = f\"Collection {DEFAULT_METADATA_SCHEMA_COLLECTION} already exists at {self.opensearch_url}\"\n",
    "            logger.info(logging_message)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def add_metadata_schema(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "        metadata_schema: list[dict[str, Any]],\n",
    "    ) -> None:\n",
    "        \"\"\"Add metadata schema to an OpenSearch index.\n",
    "\n",
    "        This method stores or updates the metadata schema definition for a specific\n",
    "        collection. It first removes any existing schema for the collection, then\n",
    "        adds the new schema definition to enable consistent metadata handling.\n",
    "\n",
    "        Args:\n",
    "            collection_name (str): The name of the collection to associate with this schema.\n",
    "            metadata_schema (list[dict[str, Any]]): List of metadata field definitions.\n",
    "                Each dictionary should contain field specifications like:\n",
    "                - name (str): The metadata field name\n",
    "                - type (str): The data type (string, integer, date, etc.)\n",
    "                - description (str): Optional field description\n",
    "\n",
    "        Returns:\n",
    "            None: This method stores the schema as a side effect.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If collection_name is invalid or metadata_schema is malformed.\n",
    "            PermissionError: If insufficient permissions to modify the metadata schema index.\n",
    "\n",
    "        Side Effects:\n",
    "            - Removes any existing metadata schema for the collection\n",
    "            - Stores the new metadata schema in the dedicated schema index\n",
    "            - Logs the schema addition with details for audit purposes\n",
    "\n",
    "        Example:\n",
    "            >>> schema = [\n",
    "            ...     {\"name\": \"author\", \"type\": \"string\", \"description\": \"Document author\"},\n",
    "            ...     {\"name\": \"created_date\", \"type\": \"date\", \"description\": \"Creation date\"}\n",
    "            ... ]\n",
    "            >>> vdb.add_metadata_schema(\"my_collection\", schema)\n",
    "            # Schema is now stored and associated with my_collection\n",
    "        \"\"\"\n",
    "        # Delete the metadata schema from the index\n",
    "        _ = self.opensearch_vs.client.delete_by_query(\n",
    "            index=DEFAULT_METADATA_SCHEMA_COLLECTION,\n",
    "            body=get_delete_metadata_schema_query(collection_name),\n",
    "        )\n",
    "        # Add the metadata schema to the index\n",
    "        data = {\n",
    "            \"collection_name\": collection_name,\n",
    "            \"metadata_schema\": metadata_schema,\n",
    "        }\n",
    "        self.opensearch_vs.client.index(index=DEFAULT_METADATA_SCHEMA_COLLECTION, body=data)\n",
    "        logger.info(\n",
    "            f\"Metadata schema added to the Opensearch index {collection_name}. Metadata schema: {metadata_schema}\"\n",
    "        )\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def get_metadata_schema(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Get the metadata schema for a collection in the OpenSearch index.\n",
    "\n",
    "        This method retrieves the stored metadata schema definition for a specific\n",
    "        collection from the dedicated metadata schema index. Returns an empty list\n",
    "        if no schema is found.\n",
    "\n",
    "        Args:\n",
    "            collection_name (str): The name of the collection to retrieve schema for.\n",
    "\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: List of metadata field definitions for the collection.\n",
    "                Each dictionary contains field specifications. Returns empty list if\n",
    "                no schema is found for the collection.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If collection_name is invalid.\n",
    "            IndexError: If the metadata schema index is corrupted or inaccessible.\n",
    "\n",
    "        Side Effects:\n",
    "            - Queries the metadata schema index\n",
    "            - Logs informational messages about schema retrieval status\n",
    "\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(opensearch_url=\"http://localhost:9200\")\n",
    "            >>> schema = vdb.get_metadata_schema(\"my_collection\")\n",
    "            >>> print(f\"Found {len(schema)} metadata fields\")\n",
    "            Found 3 metadata fields\n",
    "            >>> print(schema[0])\n",
    "            {\"name\": \"author\", \"type\": \"string\", \"description\": \"Document author\"}\n",
    "        \"\"\"\n",
    "        query = get_metadata_schema_query(collection_name)\n",
    "        response = self.opensearch_vs.client.search(\n",
    "            index=DEFAULT_METADATA_SCHEMA_COLLECTION, body=query\n",
    "        )\n",
    "        if len(response[\"hits\"][\"hits\"]) > 0:\n",
    "            return response[\"hits\"][\"hits\"][0][\"_source\"][\"metadata_schema\"]\n",
    "        else:\n",
    "            logging_message = (\n",
    "                f\"No metadata schema found for the collection: {collection_name}.\"\n",
    "                + \" Possible reason: The collection is not created with metadata schema.\"\n",
    "            )\n",
    "            logger.info(logging_message)\n",
    "            return []\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    # Methods for the VDBRag class for retrieval\n",
    "    def get_langchain_vectorstore(\n",
    "        self,\n",
    "        collection_name: str,\n",
    "    ) -> VectorStore:\n",
    "        \"\"\"Get the vectorstore for a collection.\n",
    "\n",
    "        This method creates and returns a LangChain-compatible OpenSearchVectorSearch\n",
    "        instance configured for the specified collection. The vectorstore can be used\n",
    "        directly with LangChain retrievers and chains.\n",
    "\n",
    "        Args:\n",
    "            collection_name (str): The name of the collection/index to create vectorstore for.\n",
    "\n",
    "        Returns:\n",
    "            VectorStore: A configured OpenSearchVectorSearch instance that implements\n",
    "                the LangChain VectorStore interface for semantic search operations.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If collection_name is invalid or doesn't exist.\n",
    "            ImportError: If required LangChain dependencies are not installed.\n",
    "\n",
    "        Side Effects:\n",
    "            - Creates a new vectorstore instance (lightweight operation)\n",
    "            - Validates connection to the specified OpenSearch index\n",
    "\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(opensearch_url=\"http://localhost:9200\",\n",
    "            ...                     embedding_model=embedding_model)\n",
    "            >>> vectorstore = vdb.get_langchain_vectorstore(\"my_collection\")\n",
    "            >>> retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "            >>> docs = retriever.get_relevant_documents(\"query text\")\n",
    "        \"\"\"\n",
    "        return OpenSearchVectorSearch(\n",
    "            opensearch_url=self.opensearch_url,\n",
    "            index_name=collection_name,\n",
    "            embedding_function=self.embedding_model,\n",
    "            use_ssl=False,\n",
    "        )\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    def retrieval_langchain(\n",
    "        self,\n",
    "        query: str,\n",
    "        collection_name: str,\n",
    "        vectorstore: OpenSearchVectorSearch = None,\n",
    "        top_k: int = 10,\n",
    "        filter_expr: list[dict[str, Any]] = [],\n",
    "        otel_ctx: otel_context = None,\n",
    "    ) -> list[dict[str, Any]]:\n",
    "        \"\"\"Perform semantic search and return top-k relevant documents.\n",
    "\n",
    "        This method executes semantic similarity search using LangChain's retrieval\n",
    "        framework. It supports filtering, custom vectorstore instances, and OpenTelemetry\n",
    "        tracing for observability.\n",
    "\n",
    "        Args:\n",
    "            query (str): The search query text to find similar documents for.\n",
    "            collection_name (str): The name of the collection to search in.\n",
    "            vectorstore (OpenSearchVectorSearch, optional): Pre-configured vectorstore\n",
    "                instance. If None, creates a new one for the collection.\n",
    "            top_k (int, optional): Maximum number of documents to return. Defaults to 10.\n",
    "            filter_expr (list[dict[str, Any]], optional): List of filter expressions to\n",
    "                apply during search. Each dict should contain field-value constraints.\n",
    "            otel_ctx (otel_context, optional): OpenTelemetry context for distributed tracing.\n",
    "\n",
    "        Returns:\n",
    "            list[dict[str, Any]]: List of retrieved documents with similarity scores,\n",
    "                content, and metadata. Each document includes the collection_name in\n",
    "                its metadata for multi-collection support.\n",
    "\n",
    "        Raises:\n",
    "            ConnectionError: If unable to connect to the OpenSearch cluster.\n",
    "            ValueError: If query is empty or collection_name is invalid.\n",
    "            IndexError: If the search operation fails or returns malformed results.\n",
    "\n",
    "        Side Effects:\n",
    "            - Executes similarity search against the OpenSearch index\n",
    "            - Measures and logs retrieval latency for performance monitoring\n",
    "            - Adds collection_name to each document's metadata\n",
    "            - Attaches/detaches OpenTelemetry context for tracing\n",
    "\n",
    "        Example:\n",
    "            >>> vdb = OpenSearchVDB(opensearch_url=\"http://localhost:9200\")\n",
    "            >>> filters = [{\"term\": {\"metadata.author\": \"john_doe\"}}]\n",
    "            >>> docs = vdb.retrieval_langchain(\n",
    "            ...     query=\"machine learning concepts\",\n",
    "            ...     collection_name=\"research_papers\",\n",
    "            ...     top_k=5,\n",
    "            ...     filter_expr=filters\n",
    "            ... )\n",
    "            >>> print(f\"Found {len(docs)} relevant documents\")\n",
    "        \"\"\"\n",
    "        if vectorstore is None:\n",
    "            vectorstore = self.get_langchain_vectorstore(collection_name)\n",
    "\n",
    "        if not filter_expr:\n",
    "            filter_expr = []\n",
    "\n",
    "        token = otel_context.attach(otel_ctx)\n",
    "        start_time = time.time()\n",
    "\n",
    "        retriever = vectorstore.as_retriever(\n",
    "            search_kwargs={\"k\": top_k, \"fetch_k\": top_k}\n",
    "        )\n",
    "        retriever_lambda = RunnableLambda(\n",
    "            lambda x: retriever.invoke(x, filter=filter_expr)\n",
    "        )\n",
    "        retriever_chain = {\"context\": retriever_lambda} | RunnableAssign(\n",
    "            {\"context\": lambda input: input[\"context\"]}\n",
    "        )\n",
    "        retriever_docs = retriever_chain.invoke(query, config={\"run_name\": \"retriever\"})\n",
    "        docs = retriever_docs.get(\"context\", [])\n",
    "\n",
    "        end_time = time.time()\n",
    "        latency = end_time - start_time\n",
    "        logger.info(f\" OpenSearchVectorSearch Retriever latency: {latency:.4f} seconds\")\n",
    "\n",
    "        otel_context.detach(token)\n",
    "        return self._add_collection_name_to_retreived_docs(docs, collection_name)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------\n",
    "    @staticmethod\n",
    "    def _add_collection_name_to_retreived_docs(\n",
    "        docs: list[Document], collection_name: str\n",
    "    ) -> list[Document]:\n",
    "        \"\"\"Add the collection name to the retrieved documents.\n",
    "\n",
    "        This is done to ensure the collection name is available in the\n",
    "        metadata of the documents for preparing citations in case of multi-collection retrieval.\n",
    "\n",
    "        This static utility method enhances document metadata by adding the source\n",
    "        collection name to each document. This is essential for multi-collection\n",
    "        RAG scenarios where citations need to identify the source collection.\n",
    "\n",
    "        Args:\n",
    "            docs (list[Document]): List of LangChain Document objects retrieved from search.\n",
    "                Each document should have content and metadata attributes.\n",
    "            collection_name (str): The name of the collection these documents originated from.\n",
    "                This will be added to each document's metadata.\n",
    "\n",
    "        Returns:\n",
    "            list[Document]: The same list of documents with collection_name added to\n",
    "                each document's metadata under the \"collection_name\" key.\n",
    "\n",
    "        Raises:\n",
    "            AttributeError: If documents don't have the expected metadata attribute.\n",
    "            TypeError: If docs is not a list or contains non-Document objects.\n",
    "\n",
    "        Side Effects:\n",
    "            - Modifies the metadata of each input document in-place\n",
    "            - Adds \"collection_name\" field to document metadata\n",
    "\n",
    "        Note:\n",
    "            This is a static method that can be used independently of class instances.\n",
    "            It's designed to be called after retrieval operations to prepare documents\n",
    "            for citation generation in multi-collection scenarios.\n",
    "\n",
    "        Example:\n",
    "            >>> from langchain.schema import Document\n",
    "            >>> docs = [\n",
    "            ...     Document(page_content=\"Text 1\", metadata={\"source\": \"doc1.pdf\"}),\n",
    "            ...     Document(page_content=\"Text 2\", metadata={\"source\": \"doc2.pdf\"})\n",
    "            ... ]\n",
    "            >>> enhanced_docs = OpenSearchVDB._add_collection_name_to_retreived_docs(\n",
    "            ...     docs, \"research_collection\"\n",
    "            ... )\n",
    "            >>> print(enhanced_docs[0].metadata)\n",
    "            {\"source\": \"doc1.pdf\", \"collection_name\": \"research_collection\"}\n",
    "        \"\"\"\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"collection_name\"] = collection_name\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialise the Opensearch VDBRag operator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding Model Setup:\n",
    "# Here we configure the embedding model, the neural engine that transforms your text into high-dimensional vectors.\n",
    "# Tip: Choose a model that fits your domain and scale for best results\n",
    "from nvidia_rag.utils.common import get_config\n",
    "from nvidia_rag.utils.embedding import get_embedding_model\n",
    "\n",
    "CONFIG = get_config()\n",
    "embedding_model = get_embedding_model(\n",
    "    model=CONFIG.embeddings.model_name,\n",
    "    url=CONFIG.embeddings.server_url,\n",
    "    # url=\"localhost:9080\" # TODO: Uncomment while using on-prem embeddings, and comment the above line\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Vector Database (VDB) Operator for OpenSearch\n",
    "# This operator acts as the bridge between your RAG pipeline and the underlying OpenSearch vector store.\n",
    "# Configure it here to enable semantic search, hybrid retrieval, and seamless document management.\n",
    "# Tip: Adjust the parameters below to match your OpenSearch deployment and embedding model\n",
    "\n",
    "opensearch_vdb_op = OpenSearchVDB(\n",
    "    opensearch_url=\"http://localhost:9200\",\n",
    "    index_name=\"test_library\",\n",
    "    embedding_model=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's set the required logging level. Set to INFO for displaying basic important logs. Set to DEBUG for full verbosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Set the log level via environment variable before importing nvidia_rag\n",
    "# This ensures the package respects our log level setting\n",
    "LOGLEVEL = logging.WARNING  # Set to INFO, DEBUG, WARNING or ERROR\n",
    "os.environ[\"LOGLEVEL\"] = logging.getLevelName(LOGLEVEL)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=LOGLEVEL, force=True)\n",
    "\n",
    "# Set log levels for specific loggers after package import\n",
    "for name in logging.root.manager.loggerDict:\n",
    "    if name == \"nvidia_rag\" or name.startswith(\"nvidia_rag.\"):\n",
    "        logging.getLogger(name).setLevel(LOGLEVEL)\n",
    "    if name == \"nv_ingest_client\" or name.startswith(\"nv_ingest_client.\"):\n",
    "        logging.getLogger(name).setLevel(LOGLEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Register the Opensearch VDBRag operator with `NvidiaRAG()` and `NvidiaRAGIngestor()` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia_rag import NvidiaRAG, NvidiaRAGIngestor\n",
    "\n",
    "ingestor = NvidiaRAGIngestor(\n",
    "     vdb_op=opensearch_vdb_op\n",
    ")\n",
    "\n",
    "rag = NvidiaRAG(\n",
    "    vdb_op=opensearch_vdb_op\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Utilizing Nvidia RAG Library APIs with a Opensearch Vector Database Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1. Create a new collection**\n",
    "\n",
    "Creates a new collection in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.create_collection()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2. List all collections**\n",
    "\n",
    "Retrieves all available collections from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.get_collections()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3. Add a document**\n",
    "\n",
    "Uploads new documents to the specified collection in the vector database. In case you have a requirement of updating existing documents in the specified collection, you can call `update_documents()` instead of `upload_documents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.upload_documents(\n",
    "    blocking=False,\n",
    "    split_options={\"chunk_size\": 512, \"chunk_overlap\": 150},\n",
    "    filepaths=[\n",
    "        \"../data/multimodal/woods_frost.docx\",\n",
    "        \"../data/multimodal/multimodal_test.pdf\",\n",
    "    ],\n",
    "    generate_summary=False,\n",
    ")\n",
    "task_id = response.get(\"task_id\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4. Check document upload status**\n",
    "\n",
    "Checks the status of a document upload/update task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.status(task_id=task_id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.5. [Optional] Update a document in a collection**\n",
    "\n",
    "In case you have a requirement of updating an existing document in the specified collection, execute below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.update_documents(\n",
    "    blocking=False,\n",
    "    filepaths=[\"../data/multimodal/woods_frost.docx\"],\n",
    "    generate_summary=False,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.6. Get documents in a collection**\n",
    "\n",
    "Retrieves the list of documents uploaded to a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.get_documents()\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.7. Query a document using RAG**\n",
    "\n",
    "Sends a chat-style query to the RAG system using the specified models and endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check health of all dependent services for rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "health_status_with_deps = await rag.health()\n",
    "print(json.dumps(health_status_with_deps, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "from IPython.display import Image, Markdown, display\n",
    "\n",
    "\n",
    "async def print_streaming_response_and_citations(rag_response):\n",
    "    \"\"\"\n",
    "    Print the streaming response and citations from the RAG response.\n",
    "    \"\"\"\n",
    "    # Check for API errors before processing\n",
    "    if rag_response.status_code != 200:\n",
    "        print(\"Error: \", rag_response.status_code)\n",
    "        return\n",
    "\n",
    "    # Extract the streaming generator from the response\n",
    "    response_generator = rag_response.generator\n",
    "    first_chunk_data = None\n",
    "    for chunk in response_generator:\n",
    "        if chunk.startswith(\"data: \"):\n",
    "            chunk = chunk[len(\"data: \") :].strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            continue\n",
    "        choices = data.get(\"choices\", [])\n",
    "        if not choices:\n",
    "            continue\n",
    "        # Save the first chunk with citations\n",
    "        if first_chunk_data is None and data.get(\"citations\"):\n",
    "            first_chunk_data = data\n",
    "        # Print streaming text\n",
    "        delta = choices[0].get(\"delta\", {})\n",
    "        text = delta.get(\"content\")\n",
    "        if not text:\n",
    "            message = choices[0].get(\"message\", {})\n",
    "            text = message.get(\"content\", \"\")\n",
    "        print(text, end=\"\", flush=True)\n",
    "    print()  # Newline after streaming\n",
    "\n",
    "    # Display citations after streaming is done\n",
    "    if first_chunk_data and first_chunk_data.get(\"citations\"):\n",
    "        citations = first_chunk_data[\"citations\"]\n",
    "        for idx, citation in enumerate(citations.get(\"results\", [])):\n",
    "            doc_type = citation.get(\"document_type\", \"text\")\n",
    "            content = citation.get(\"content\", \"\")\n",
    "            doc_name = citation.get(\"document_name\", f\"Citation {idx + 1}\")\n",
    "            display(Markdown(f\"**Citation {idx + 1}: {doc_name}**\"))\n",
    "            try:\n",
    "                image_bytes = base64.b64decode(content)\n",
    "                display(Image(data=image_bytes))\n",
    "            except Exception:\n",
    "                display(Markdown(f\"```\\n{content}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the generate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "await print_streaming_response_and_citations(\n",
    "    rag.generate(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is the price of a hammer?\"}],\n",
    "        use_knowledge_base=True,\n",
    "        collection_names=[\"test_library\"], # Kindly provide collection_names argument\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.8. [Optional] Search for documents**\n",
    "\n",
    "Performs a search in the vector database for relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_citations(citations):\n",
    "    \"\"\"\n",
    "    Display all citations from the Citations object returned by search().\n",
    "    Handles base64-encoded images and text.\n",
    "    \"\"\"\n",
    "    if not citations or not hasattr(citations, \"results\") or not citations.results:\n",
    "        print(\"No citations found.\")\n",
    "        return\n",
    "\n",
    "    for idx, citation in enumerate(citations.results):\n",
    "        # If using pydantic models, citation fields may be attributes, not dict keys\n",
    "        doc_type = getattr(citation, \"document_type\", \"text\")\n",
    "        content = getattr(citation, \"content\", \"\")\n",
    "        doc_name = getattr(citation, \"document_name\", f\"Citation {idx + 1}\")\n",
    "\n",
    "        display(Markdown(f\"**Citation {idx + 1}: {doc_name}**\"))\n",
    "        try:\n",
    "            image_bytes = base64.b64decode(content)\n",
    "            display(Image(data=image_bytes))\n",
    "        except Exception:\n",
    "            display(Markdown(f\"```\\n{content}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the search API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_search_citations(\n",
    "    rag.search(\n",
    "        query=\"What is the price of a hammer?\",\n",
    "        collection_names=[\"test_library\"], # Kindly provide collection_names argument\n",
    "        reranker_top_k=10,\n",
    "        vdb_top_k=100,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.9. [Optional] Retrieve documents summary**\n",
    "\n",
    "You can execute this cell if summary generation was enabled during document upload using `generate_summary: bool` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await rag.get_summary(\n",
    "    collection_name=\"test_library\", # Kindly provide collection_names argument\n",
    "    file_name=\"woods_frost.docx\",\n",
    "    blocking=False,\n",
    "    timeout=20,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** Below APIs illustrate how to cleanup uploaded documents and collections once no more interaction is needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.10. Delete documents from a collection**\n",
    "\n",
    "Deletes documents from the specified collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.delete_documents(\n",
    "    document_names=[\"../data/multimodal/multimodal_test.pdf\"],\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.11. Delete collections**\n",
    "\n",
    "Deletes the specified collection and all its documents from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.delete_collections(\n",
    "    collection_names=[\"test_library\"]\n",
    ")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-library",
   "language": "python",
   "name": "rag-library"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
