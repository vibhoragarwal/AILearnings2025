{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA RAG Python Package\n",
    "\n",
    "This notebook demonstrates how to use the Nvidia RAG Python client for document ingestion, collection management, and querying.\n",
    "\n",
    "## Installation guide for python package\n",
    "\n",
    "Before running the cells below, follow these steps in your terminal from the project root directory to install the python package in your environment and launch this notebook:\n",
    "\n",
    "> **Note**: Python version **3.12 or higher** is supported.\n",
    "\n",
    "```bash\n",
    "# 1. Install Python >= 3.12 (e.g., Python 3.13) and its development headers\n",
    "    sudo add-apt-repository ppa:deadsnakes/ppa\n",
    "    sudo apt update\n",
    "    sudo apt install python3.12\n",
    "    sudo apt-get install python3.12-dev\n",
    "\n",
    "# 2. Install uv\n",
    "Follow instruction from https://docs.astral.sh/uv/getting-started/installation/\n",
    "\n",
    "# 3. Create a virtual environment with a supported Python version (>= 3.12)\n",
    "uv venv --python=python3.12\n",
    "\n",
    "# 2. Activate the virtual environment\n",
    "source .venv/bin/activate\n",
    "\n",
    "# 3. (Option 1) Build the wheel from source and install the Nvidia RAG wheel\n",
    "uv build\n",
    "uv pip install dist/nvidia_rag-2.3.0.dev0-py3-none-any.whl[all]\n",
    "\n",
    "# 4. (Option 2) Install the package in editable (development) mode from source\n",
    "uv pip install -e .[all]\n",
    "\n",
    "# 5. (Option 3) Install the prebuilt wheel file from pypi. This does not require you to clone the repo.\n",
    "uv pip install nvidia-rag[all]\n",
    "\n",
    "# 5. Start the notebook server and open this notebook in browser \n",
    "uv pip install jupyterlab\n",
    "jupyter lab --allow-root --ip=0.0.0.0 --NotebookApp.token='' --port=8889 --no-browser &\n",
    "Open http://<workstation_ip>:8889/lab/tree/notebooks\n",
    "\n",
    "# 6. Optional: Install just RAG and Ingestor dependencies\n",
    "uv pip install dist/nvidia_rag-2.3.0.dev0-py3-none-any.whl[rag]\n",
    "uv pip install dist/nvidia_rag-2.3.0.dev0-py3-none-any.whl[ingest]\n",
    "```\n",
    "\n",
    "##### üìù **Note:**\n",
    "\n",
    "- Installing with `uv pip install -e .[all]` allows you to make live edits to the `nvidia_rag` source code and have those changes reflected without reinstalling the package.\n",
    "- **After making changes to the source code, you need to:\n",
    "  - Restart the kernel of your notebook server\n",
    "  - Re-execute the cells `Setup the default configurations` under `Setting up the dependencies` and `Import the packages` under `API usage examples`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify the installation\n",
    "The location of the package shown in the output of this command should be inside the virtual environment.\n",
    "\n",
    "Location: `<workspace_path>/rag/.venv/lib/python3.12/site-packages`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip show nvidia_rag | grep Location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the environment for the python package is setup we now launch all the dependent services and NIMs the pipeline depends on.\n",
    "Fulfill the [prerequisites here](../docs/deploy-docker-self-hosted.md) to setup docker on your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup the default configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install python-dotenv\n",
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide your NGC_API_KEY after executing the cell below. You can obtain a key by following steps [here](../docs/api-key.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del os.environ['NVIDIA_API_KEY']  ## delete key and reset if needed\n",
    "if os.environ.get(\"NGC_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    print(\"Valid NGC_API_KEY already in environment. Delete to reset\")\n",
    "else:\n",
    "    candidate_api_key = getpass(\"NVAPI Key (starts with nvapi-): \")\n",
    "    assert candidate_api_key.startswith(\"nvapi-\"), (\n",
    "        f\"{candidate_api_key[:5]}... is not a valid key\"\n",
    "    )\n",
    "    os.environ[\"NGC_API_KEY\"] = candidate_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Login to nvcr.io which is needed for pulling the containers of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"${NGC_API_KEY}\" | docker login nvcr.io -u '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the default values for all the configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(dotenv_path=\".env_library\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*üí° **Tip:***: You can override any default values of configurations defined in `.env_library` at runtime by using `os.environ` in the notebook. Reimport the `nvidia_rag` package and restart the  Nvidia Ingest runtime to take in the updated configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# os.environ[\"ENV_VAR_NAME\"]=\"ENV_VAR_VALUE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setup the Milvus vector DB services\n",
    "By default milvus uses GPU Indexing. Ensure you have provided correct GPU ID.\n",
    "Note: If you don't have a GPU available, you can switch to CPU-only Milvus by following the instructions in [milvus-configuration.md](../docs/milvus-configuration.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"VECTORSTORE_GPU_DEVICE_ID\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f ../deploy/compose/vectordb.yaml up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Setup the NIMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 1: Deploy on-prem models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move to Option 2 if you are interested in using cloud models.\n",
    "\n",
    "Ensure you meet [the hardware requirements](../docs/support-matrix.md). By default the NIMs are configured to use 2xH100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model cache directory\n",
    "!mkdir -p ~/.cache/model-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the MODEL_DIRECTORY environment variable in the Python kernel\n",
    "import os\n",
    "\n",
    "os.environ[\"MODEL_DIRECTORY\"] = os.path.expanduser(\"~/.cache/model-cache\")\n",
    "print(\"MODEL_DIRECTORY set to:\", os.environ[\"MODEL_DIRECTORY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure GPU IDs for the various microservices if needed\n",
    "os.environ[\"EMBEDDING_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"RANKING_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_GRAPHICS_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"YOLOX_TABLE_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"OCR_MS_GPU_ID\"] = \"0\"\n",
    "os.environ[\"LLM_MS_GPU_ID\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select your hardware-specific profile name as per the guidance provided in [NIM Model Profile Configuration](../docs/model-profiles.md) section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"NIM_MODEL_PROFILE\"] = \"......\" # Populate your profile name as per hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚ö†Ô∏è Deploying NIMs - This may take a while as models download. If kernel times out, just rerun this cell.\n",
    "!USERID=$(id -u) docker compose -f ../deploy/compose/nims.yaml up -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch the status of running containers (run this cell repeatedly or in a terminal)\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure all the below are running and healthy before proceeding further\n",
    "```output\n",
    "NAMES                           STATUS\n",
    "nemoretriever-ranking-ms        Up ... (healthy)\n",
    "compose-page-elements-1         Up ...\n",
    "compose-paddle-1                Up ...\n",
    "compose-graphic-elements-1      Up ...\n",
    "compose-table-structure-1       Up ...\n",
    "nemoretriever-embedding-ms      Up ... (healthy)\n",
    "nim-llm-ms                      Up ... (healthy)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option 2: Using Nvidia Hosted models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"APP_LLM_MODELNAME\"] = \"nvidia/llama-3.3-nemotron-super-49b-v1.5\"\n",
    "os.environ[\"APP_EMBEDDINGS_MODELNAME\"] = \"nvidia/llama-3.2-nv-embedqa-1b-v2\"\n",
    "os.environ[\"APP_RANKING_MODELNAME\"] = \"nvidia/llama-3.2-nv-rerankqa-1b-v2\"\n",
    "os.environ[\"APP_EMBEDDINGS_SERVERURL\"] = \"\"\n",
    "os.environ[\"APP_LLM_SERVERURL\"] = \"\"\n",
    "os.environ[\"APP_RANKING_SERVERURL\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/retrieval/nvidia/llama-3_2-nv-rerankqa-1b-v2/reranking/v1\"\n",
    ")\n",
    "os.environ[\"OCR_HTTP_ENDPOINT\"] = \"https://ai.api.nvidia.com/v1/cv/baidu/paddleocr\"\n",
    "os.environ[\"OCR_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-page-elements-v2\"\n",
    ")\n",
    "os.environ[\"YOLOX_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-graphic-elements-v1\"\n",
    ")\n",
    "os.environ[\"YOLOX_GRAPHIC_ELEMENTS_INFER_PROTOCOL\"] = \"http\"\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_HTTP_ENDPOINT\"] = (\n",
    "    \"https://ai.api.nvidia.com/v1/cv/nvidia/nemoretriever-table-structure-v1\"\n",
    ")\n",
    "os.environ[\"YOLOX_TABLE_STRUCTURE_INFER_PROTOCOL\"] = \"http\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Setup the Nvidia Ingest runtime and redis service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f ../deploy/compose/docker-compose-ingestor-server.yaml up nv-ingest-ms-runtime redis -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Load optional profiles if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load accuracy profile\n",
    "# load_dotenv(dotenv_path='../deploy/compose/accuracy_profile.env', override=True)\n",
    "\n",
    "# OR load perf profile\n",
    "# load_dotenv(dotenv_path='../deploy/compose/perf_profile.env', override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# API usage example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting up the python package and starting all dependent services, finally we can execute some snippets showcasing all different functionalities offered by the `nvidia_rag` package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set logging level\n",
    "First let's set the required logging level. Set to INFO for displaying basic important logs. Set to DEBUG for full verbosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "# Set the log level via environment variable before importing nvidia_rag\n",
    "# This ensures the package respects our log level setting\n",
    "LOGLEVEL = logging.WARNING  # Set to INFO, DEBUG, WARNING or ERROR\n",
    "os.environ[\"LOGLEVEL\"] = logging.getLevelName(LOGLEVEL)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=LOGLEVEL, force=True)\n",
    "\n",
    "# Set log levels for specific loggers after package import\n",
    "for name in logging.root.manager.loggerDict:\n",
    "    if name == \"nvidia_rag\" or name.startswith(\"nvidia_rag.\"):\n",
    "        logging.getLogger(name).setLevel(LOGLEVEL)\n",
    "    if name == \"nv_ingest_client\" or name.startswith(\"nv_ingest_client.\"):\n",
    "        logging.getLogger(name).setLevel(LOGLEVEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the packages\n",
    "You can import both or either one based on your requirements. `NvidiaRAG()` exposes APIs to interact with the uploaded documents and `NvidiaRAGIngestor()` exposes APIs for document upload and management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia_rag import NvidiaRAG, NvidiaRAGIngestor\n",
    "\n",
    "rag = NvidiaRAG()\n",
    "ingestor = NvidiaRAGIngestor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create a new collection\n",
    "Creates a new collection in the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.create_collection(\n",
    "    collection_name=\"test_library\",\n",
    "    vdb_endpoint=\"http://localhost:19530\",\n",
    "    # [Optional]: Create collection with metadata schema, uncomment to create collection with metadata schemas\n",
    "    # metadata_schema = [\n",
    "    #     {\n",
    "    #         \"name\": \"meta_field_1\",\n",
    "    #         \"type\": \"string\",\n",
    "    #         \"description\": \"Following field would contain the description for the document\"\n",
    "    #     }\n",
    "    # ]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. List all collections\n",
    "Retrieves all available collections from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.get_collections(vdb_endpoint=\"http://localhost:19530\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Add a document\n",
    "Uploads new documents to the specified collection in the vector database. In case you have a requirement of updating existing documents in the specified collection, you can call `update_documents()` instead of `upload_documents()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.upload_documents(\n",
    "    collection_name=\"test_library\",\n",
    "    vdb_endpoint=\"http://localhost:19530\",\n",
    "    blocking=False,\n",
    "    split_options={\"chunk_size\": 512, \"chunk_overlap\": 150},\n",
    "    filepaths=[\n",
    "        \"../data/multimodal/woods_frost.docx\",\n",
    "        \"../data/multimodal/multimodal_test.pdf\",\n",
    "    ],\n",
    "    generate_summary=False,\n",
    "    # [Optional]: Uncomment to add custom metadata, ensure that the metadata schema is created with the same fields with create_collection\n",
    "    # custom_metadata=[\n",
    "    #     {\n",
    "    #         \"filename\": \"multimodal_test.pdf\",\n",
    "    #         \"metadata\": {\"meta_field_1\": \"multimodal document 1\"}\n",
    "    #     },\n",
    "    #     {\n",
    "    #         \"filename\": \"woods_frost.docx\",\n",
    "    #         \"metadata\": {\"meta_field_1\": \"multimodal document 2\"}\n",
    "    #     }\n",
    "    # ]\n",
    ")\n",
    "task_id = response.get(\"task_id\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Check document upload status\n",
    "Checks the status of a document upload/update task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.status(task_id=task_id)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  [Optional] Update a document in a collection\n",
    "In case you have a requirement of updating an existing document in the specified collection, execute below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await ingestor.update_documents(\n",
    "    collection_name=\"test_library\",\n",
    "    vdb_endpoint=\"http://localhost:19530\",\n",
    "    blocking=False,\n",
    "    filepaths=[\"../data/multimodal/woods_frost.docx\"],\n",
    "    generate_summary=False,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Get documents in a collection\n",
    "Retrieves the list of documents uploaded to a collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.get_documents(\n",
    "    collection_name=\"test_library\",\n",
    "    vdb_endpoint=\"http://localhost:19530\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Query a document using RAG\n",
    "Sends a chat-style query to the RAG system using the specified models and endpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check health of all dependent services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "health_status_with_deps = await rag.health()\n",
    "print(json.dumps(health_status_with_deps, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import json\n",
    "\n",
    "from IPython.display import Image, Markdown, display\n",
    "\n",
    "\n",
    "async def print_streaming_response_and_citations(rag_response):\n",
    "    \"\"\"\n",
    "    Print the streaming response and citations from the RAG response.\n",
    "    \"\"\"\n",
    "    # Check for API errors before processing\n",
    "    if rag_response.status_code != 200:\n",
    "        print(\"Error: \", rag_response.status_code)\n",
    "        return\n",
    "\n",
    "    # Extract the streaming generator from the response\n",
    "    response_generator = rag_response.generator\n",
    "    first_chunk_data = None\n",
    "    for chunk in response_generator:\n",
    "        if chunk.startswith(\"data: \"):\n",
    "            chunk = chunk[len(\"data: \") :].strip()\n",
    "        if not chunk:\n",
    "            continue\n",
    "        try:\n",
    "            data = json.loads(chunk)\n",
    "        except Exception as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "            continue\n",
    "        choices = data.get(\"choices\", [])\n",
    "        if not choices:\n",
    "            continue\n",
    "        # Save the first chunk with citations\n",
    "        if first_chunk_data is None and data.get(\"citations\"):\n",
    "            first_chunk_data = data\n",
    "        # Print streaming text\n",
    "        delta = choices[0].get(\"delta\", {})\n",
    "        text = delta.get(\"content\")\n",
    "        if not text:\n",
    "            message = choices[0].get(\"message\", {})\n",
    "            text = message.get(\"content\", \"\")\n",
    "        print(text, end=\"\", flush=True)\n",
    "    print()  # Newline after streaming\n",
    "\n",
    "    # Display citations after streaming is done\n",
    "    if first_chunk_data and first_chunk_data.get(\"citations\"):\n",
    "        citations = first_chunk_data[\"citations\"]\n",
    "        for idx, citation in enumerate(citations.get(\"results\", [])):\n",
    "            doc_type = citation.get(\"document_type\", \"text\")\n",
    "            content = citation.get(\"content\", \"\")\n",
    "            doc_name = citation.get(\"document_name\", f\"Citation {idx + 1}\")\n",
    "            display(Markdown(f\"**Citation {idx + 1}: {doc_name}**\"))\n",
    "            try:\n",
    "                image_bytes = base64.b64decode(content)\n",
    "                display(Image(data=image_bytes))\n",
    "            except Exception:\n",
    "                display(Markdown(f\"```\\n{content}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await print_streaming_response_and_citations(\n",
    "    rag.generate(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is the price of a hammer?\"}],\n",
    "        use_knowledge_base=True,\n",
    "        collection_names=[\"test_library\"],\n",
    "        # embedding_endpoint=\"localhost:9080\", # TODO: Uncomment while using on-prem embeddings\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Search for documents\n",
    "Performs a search in the vector database for relevant documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_citations(citations):\n",
    "    \"\"\"\n",
    "    Display all citations from the Citations object returned by search().\n",
    "    Handles base64-encoded images and text.\n",
    "    \"\"\"\n",
    "    if not citations or not hasattr(citations, \"results\") or not citations.results:\n",
    "        print(\"No citations found.\")\n",
    "        return\n",
    "\n",
    "    for idx, citation in enumerate(citations.results):\n",
    "        # If using pydantic models, citation fields may be attributes, not dict keys\n",
    "        doc_type = getattr(citation, \"document_type\", \"text\")\n",
    "        content = getattr(citation, \"content\", \"\")\n",
    "        doc_name = getattr(citation, \"document_name\", f\"Citation {idx + 1}\")\n",
    "\n",
    "        display(Markdown(f\"**Citation {idx + 1}: {doc_name}**\"))\n",
    "        try:\n",
    "            image_bytes = base64.b64decode(content)\n",
    "            display(Image(data=image_bytes))\n",
    "        except Exception:\n",
    "            display(Markdown(f\"```\\n{content}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_search_citations(\n",
    "    rag.search(\n",
    "        query=\"What is the price of a hammer?\",\n",
    "        collection_names=[\"test_library\"],\n",
    "        reranker_top_k=10,\n",
    "        vdb_top_k=100,\n",
    "        # embedding_endpoint=\"localhost:9080\" # TODO: Uncomment while using on-prem embeddings\n",
    "        # [Optional]: Uncomment to filter the documents based on the metadata, ensure that the metadata schema is created with the same fields with create_collection\n",
    "        # filter_expr='content_metadata[\"meta_field_1\"] == \"multimodal document 1\"'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. [Optional] Retrieve documents summary\n",
    "You can execute this cell if summary generation was enabled during document upload using `generate_summary: bool` flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await rag.get_summary(\n",
    "    collection_name=\"test_library\",\n",
    "    file_name=\"woods_frost.docx\",\n",
    "    blocking=False,\n",
    "    timeout=20,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below APIs illustrate how to cleanup uploaded documents and collections once no more interaction is needed.\n",
    "## 9. Delete documents from a collection\n",
    "Deletes documents from the specified collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.delete_documents(\n",
    "    collection_name=\"test_library\",\n",
    "    document_names=[\"../data/multimodal/multimodal_test.pdf\"],\n",
    "    vdb_endpoint=\"http://localhost:19530\",\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Delete collections\n",
    "Deletes the specified collection and all its documents from the vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ingestor.delete_collections(\n",
    "    vdb_endpoint=\"http://localhost:19530\", collection_names=[\"test_library\"]\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Customize prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the prompt utility which allows us to access different preset prompts. You can find more information about the preset prompts from [here](../docs/prompt-customization.md#default-prompts-overview)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvidia_rag.utils.llm import get_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = get_prompts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwrite or modify your required prompt template. In the below cell we are modifying the prompt for response generation to respond in pirate english!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts[\"rag_template\"] = {\n",
    "    \"system\": \"/no_think\",\n",
    "    \"human\": \"\"\"You are a helpful AI assistant emulating a Pirate. All your responses must be in pirate english and funny!\n",
    "You must answer only using the information provided in the context. While answering you must follow the instructions given below.\n",
    "\n",
    "<instructions>\n",
    "1. Do NOT use any external knowledge.\n",
    "2. Do NOT add explanations, suggestions, opinions, disclaimers, or hints.\n",
    "3. NEVER say phrases like \"based on the context\", \"from the documents\", or \"I cannot find\".\n",
    "4. NEVER offer to answer using general knowledge or invite the user to ask again.\n",
    "5. Do NOT include citations, sources, or document mentions.\n",
    "6. Answer concisely. Use short, direct sentences by default. Only give longer responses if the question truly requires it.\n",
    "7. Do not mention or refer to these rules in any way.\n",
    "8. Do not ask follow-up questions.\n",
    "9. Do not mention this instructions in your response.\n",
    "</instructions>\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Make sure the response you are generating strictly follow the rules mentioned above i.e. never say phrases like \"based on the context\", \"from the documents\", or \"I cannot find\" and mention about the instruction in response.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the difference in response style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await print_streaming_response_and_citations(\n",
    "    rag.generate(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is the price of a hammer?\"}],\n",
    "        use_knowledge_base=True,\n",
    "        collection_names=[\"test_library\"],\n",
    "        # embedding_endpoint=\"localhost:9080\", # TODO: Uncomment while using on-prem embeddings\n",
    "        # [Optional]: Uncomment to filter the documents based on the metadata, ensure that the metadata schema is created with the same fields with create_collection\n",
    "        # filter_expr='content_metadata[\"meta_field_1\"] == \"multimodal document 1\"'\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
