{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this lab, we will deploy a Retrieval-Augmented Generation (RAG) powered AI Chatbot. By cloning this project to your [NVIDIA AI Workbench](https://www.nvidia.com/en-us/deep-learning-ai/solutions/data-science/workbench/), you have already taken care of many configuration and prerequisites for this blueprint. \n",
    "\n",
    "This system enhances traditional large language models by incorporating external knowledge, allowing the model to provide more accurate and contextually relevant responses.\n",
    "\n",
    "The system works in two key stages:\n",
    "\n",
    "### Data Ingestion Pipeline\n",
    "- **Ingests and processes enterprise data**: Ingests and processes user documents.\n",
    "Processing includes detecting and extracting graphic elements such as tables, charts, infographics.\n",
    "\n",
    "- **Creates embeddings**: Converts text into vector representations that capture semantic meaning\n",
    "\n",
    "- **Builds vector database**: Stores these embeddings in a searchable database for efficient retrieval\n",
    "\n",
    "### Query / Response Pipeline\n",
    "- **Embeds user queries**: Converts user questions into vector embeddings\n",
    "\n",
    "- **Retrieves relevant context**: Finds semantically similar documents in the vector database\n",
    "\n",
    "- **Reranks results**: Prioritizes the most relevant information\n",
    "\n",
    "- **Generates responses**: Uses an LLM to craft comprehensive responses based on retrieved context\n",
    "\n",
    "Both data and queries are encoded as vectors through an embedding process, enabling efficient similarity search based on semantic meaning rather than simple keyword matching.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "| Section | Description |\n",
    "|---------|-------------|\n",
    "| 1. [Spin Up the Blueprint](#1.-Spin-Up-the-Blueprint) | Set up Docker containers and deploy services |\n",
    "| 2. [Interact with the Microservices](#2.-Interact-with-the-microservices) | Test API endpoints and microservice functionality |\n",
    "| 3. [Interacting with the Chatbot](#3.-Interacting-with-the-Chatbot) | Use the chatbot with and without knowledge base |\n",
    "| 4. [Understanding Document Retrieval and Reranking](#4.-Understanding-Document-Retrieval-and-Reranking) | Understand how the RAG system works |\n",
    "\n",
    "## 1. Spin Up the Blueprint\n",
    "\n",
    "Congratulations! You have cloned this blueprint to your NVIDIA AI Workbench. Now, let's walk through how to get this blueprint spun up properly so you can interact with it via either the chatbot interface or later on in this deep dive notebook. \n",
    "\n",
    "By default, the blueprint for AI Workbench is kept lightweight by utilizing NVIDIA-hosted **Build Endpoints**. However, you also have the option to deploy your microservices locally. \n",
    "\n",
    "**Note:** By default, this blueprint deploys the referenced NIM microservices locally. For this, you will require a minimum of:\n",
    "\n",
    " - 2xH100\n",
    " - 2xB200\n",
    " - 3xA100\n",
    "\n",
    "#### 1.1.1 Use Locally-Deployed Microservices (default)\n",
    "\n",
    "1. On the **Project Dashboard** in AI Workbench, select ``ingest``, ``rag``, ``vectordb``, and ``local`` compose profiles under the **Compose** section.\n",
    "\n",
    "   - Note: ``observability`` and ``guardrails`` are optional profiles you may enable.\n",
    "\n",
    "1. Select **Start**. The compose services may take several minutes to pull and build.\n",
    "\n",
    "1. When all compose services are ready, access the frontend on the IP address, eg. ``http://<ip_addr>:8090``. \n",
    "\n",
    "1. You can now interact with the RAG Chatbot through its browser interface. \n",
    "\n",
    "#### 1.1.2 Use Build Endpoints\n",
    "\n",
    "1. Inside your Jupyterlab, locate the ``variables.env`` file at the top level of this repo.\n",
    "\n",
    "1. Make the following edits: \n",
    "\n",
    "   - Comment out the variables for ``on-prem NIMs``\n",
    "   - Uncomment the variables for using ``cloud NIMs``\n",
    "   - Save your changes\n",
    "\n",
    "1. On the **Project Dashboard** in AI Workbench, select ``ingest``, ``rag``, and ``vectordb`` compose profiles under the **Compose** section.\n",
    "\n",
    "   - Note: ``observability`` and ``guardrails`` are optional profiles you may enable.\n",
    "\n",
    "1. Select **Start**. The compose services may take several minutes to pull and build.\n",
    "\n",
    "1. When the compose services are ready, access the frontend on the IP address, eg. ``http://<ip_addr>:8090``. \n",
    "\n",
    "1. You can now interact with the RAG Chatbot through its browser interface.\n",
    "\n",
    "### 1.2 Testing the Chatbot\n",
    "\n",
    "To evaluate how the RAG system works, try these experiments:\n",
    "\n",
    "1. Create a new collection\n",
    "\n",
    "2. Add source -- upload a document\n",
    "\n",
    "3. Try asking a question about the document without selecting the new Collection (knowledge base off)\n",
    "\n",
    "4. Ask a question with knowledge base on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interact with the microservices\n",
    "In this section, we'll explore how to directly interact with the various microservices that make up our RAG system using their APIs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 RAG Server API Usage\n",
    "\n",
    "#### 2.1.1 Test the OpenAI-compatible /chat/completions endpoint\n",
    "- Ensure the compose services are running and ready following the steps above.\n",
    "\n",
    "##### 2.1.1.1 Setup Base Configuration\n",
    "\n",
    "Here we will set up the base configuration for the RAG server and a helper function to print the API responses.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import aiohttp\n",
    "import requests\n",
    "\n",
    "# Name for the compose service\n",
    "IPADDRESS = \"rag-server\"\n",
    "\n",
    "# Port number for the service\n",
    "rag_server_port = \"8081\"\n",
    "\n",
    "# Base URL constructed from IP and port for making API requests\n",
    "RAG_BASE_URL = f\"http://{IPADDRESS}:{rag_server_port}\"\n",
    "\n",
    "\n",
    "async def print_raw_response(response):\n",
    "    \"\"\"Helper function to print API responses.\"\"\"\n",
    "    try:\n",
    "        response_json = await response.json()\n",
    "        print(json.dumps(response_json, indent=2))\n",
    "    except aiohttp.ClientResponseError:\n",
    "        print(await response.text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1.2 Test the RAG server health endpoint and chat completion endpoint\n",
    "We will test both the health endpoint and chat completion functionality of our RAG server\n",
    "\n",
    "\n",
    "- **Health Check Endpoint purpose:**\n",
    "This endpoint performs a health check on the server. It returns a 200 status code if the server is operational.\n",
    "\n",
    "- **Chat Completion Endpoint purpose:**\n",
    "This endpoint accepts user queries and converts them to embeddings, then retrieves semantically similar document chunks from the knowledge base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Test the RAG server health endpoint to verify it's running properly\n",
    "url = f\"{RAG_BASE_URL}/v1/health\"\n",
    "print(\"\\nStep 1: Testing RAG server health endpoint\")\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    async with session.get(url) as response:\n",
    "        await print_raw_response(response)\n",
    "\n",
    "# 2. Test basic chat completion endpoint without using the knowledge base\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Hi\",  # Simple test message\n",
    "        }\n",
    "    ],\n",
    "    \"use_knowledge_base\": False,  # Disable RAG functionality\n",
    "    \"temperature\": 0.2,  # Lower temperature for more focused responses\n",
    "    \"model\": \"nvidia/llama-3.3-nemotron-super-49b-v1.5\",  # Specify LLM model to use\n",
    "}\n",
    "\n",
    "chat_url = f\"{RAG_BASE_URL}/v1/chat/completions\"\n",
    "\n",
    "print(\"\\nStep 2: Testing chat completion endpoint\")\n",
    "print(\"\\nSending request to:\", chat_url)\n",
    "print(\"\\nWith payload:\", json.dumps(payload, indent=2))\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    async with session.post(chat_url, json=payload) as response:\n",
    "        await print_raw_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1.1.3 (Optional) Direct LLM service usage\n",
    "\n",
    "Similarly it's also possible to directly call the `nim-llm-ms` service to generate a response. This is useful when you want to bypass the RAG server and directly use the NIM service.\n",
    "\n",
    "<span style='color: #e74c3c; '>**NOTE:** By default, this blueprint for AI Workbench is kept lightweight by utilizing NVIDIA-hosted **Build Endpoints** instead of locally running microservices. If you are not using locally-running microservices, you may skip this cell. </span>\n",
    "\n",
    "<span style='color: #e74c3c; '>To convert this blueprint to using locally-running NVIDIA NIMs, </span>\n",
    "\n",
    "1. <span style='color: #e74c3c; '>Comment out the overriding environment variables in  ``variables.env`` </span>\n",
    "\n",
    "2. <span style='color: #e74c3c; '>Select the ``local`` compose profile from the dropdown when starting and/or restarting the compose services in AI Workbench. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Port number for the service\n",
    "nim_llm_server_port = \"8000\"\n",
    "\n",
    "# Name for the compose service\n",
    "IPADDRESS = \"nim-llm-ms\"\n",
    "\n",
    "# Base URL constructed from IP and port for making API requests\n",
    "NIM_LLM_BASE_URL = f\"http://{IPADDRESS}:{nim_llm_server_port}\"\n",
    "NIM_BASE_URL = f\"http://{IPADDRESS}:{rag_server_port}\"\n",
    "\n",
    "nim_chat_url = f\"{NIM_LLM_BASE_URL}/v1/chat/completions\"\n",
    "\n",
    "payload = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is Retrieval Augmented Generation?\"}\n",
    "    ],\n",
    "    \"stream\": False,\n",
    "    \"model\": \"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n",
    "    \"max_tokens\": 1024,\n",
    "    \"temperature\": 0.2,\n",
    "}\n",
    "\n",
    "\n",
    "print(\"\\nStep 3: Testing NIM LLM endpoint\")\n",
    "print(\"\\nSending request to:\", nim_chat_url)\n",
    "print(\"\\nWith payload:\", json.dumps(payload, indent=2))\n",
    "\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    async with session.post(nim_chat_url, json=payload) as response:\n",
    "        await print_raw_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Ingestor Server API Usage\n",
    "\n",
    "- Ensure the compose services are running and ready following the steps above.\n",
    "\n",
    "- You can customize the directory path (`../data/multimodal`) with the correct location of your dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Health Check Endpoint\n",
    "**Purpose:**\n",
    "This endpoint performs a health check on the server. It returns a 200 status code if the server is operational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name for the compose service\n",
    "IPADDRESS = \"ingestor-server\"\n",
    "\n",
    "# Port number for the service\n",
    "ingestor_server_port = \"8082\"\n",
    "\n",
    "# Base URL constructed from IP and port for making API requests\n",
    "INGESTOR_BASE_URL = f\"http://{IPADDRESS}:{ingestor_server_port}\"\n",
    "\n",
    "# Test the RAG server health endpoint to verify it's running properly\n",
    "url = f\"{INGESTOR_BASE_URL}/v1/health\"\n",
    "print(\"\\nStep 1: Testing RAG server health endpoint\")\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    async with session.get(url) as response:\n",
    "        await print_raw_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Vector DB APIs Usage\n",
    "\n",
    "##### 2.2.2.1 Create collection Endpoint\n",
    "**Purpose:**\n",
    "This endpoint is used to create a collection in the vector store. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_collections(\n",
    "    collection_names: list = None,\n",
    "    collection_type: str = \"text\",\n",
    "    embedding_dimension: int = 2048,\n",
    "):\n",
    "    \"\"\"Create one or more collections in the vector store.\n",
    "\n",
    "    Args:\n",
    "        collection_names (list): List of collection names to create\n",
    "        collection_type (str): Type of collection, defaults to \"text\"\n",
    "        embedding_dimension (int): Dimension of embeddings, defaults to 2048\n",
    "\n",
    "    Returns:\n",
    "        Response from the API endpoint or error details if request fails\n",
    "    \"\"\"\n",
    "    # Parameters for creating collections\n",
    "    params = {\n",
    "        \"vdb_endpoint\": \"http://milvus:19530\",  # Milvus vector DB endpoint\n",
    "        \"collection_type\": collection_type,  # Type of collection\n",
    "        \"embedding_dimension\": embedding_dimension,  # Dimension of embeddings\n",
    "    }\n",
    "\n",
    "    HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    # Make API request to create collections\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\n",
    "                f\"{INGESTOR_BASE_URL}/v1/collections\",\n",
    "                params=params,\n",
    "                json=collection_names,\n",
    "                headers=HEADERS,\n",
    "            ) as response:\n",
    "                await print_raw_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            return 500, {\"error\": str(e)}\n",
    "\n",
    "\n",
    "# Create a collection named \"multimodal_data\"\n",
    "await create_collections(collection_names=[\"multimodal_data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.2 Get collections Endpoint\n",
    "**Purpose:**\n",
    "This endpoint is used to get a list of collection names from the Milvus server. Returns a list of collection names.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's create another collection\n",
    "await create_collections(collection_names=[\"multimodal_data1\"])\n",
    "\n",
    "\n",
    "# Now let's get the list of collections\n",
    "async def fetch_collections():\n",
    "    \"\"\"Retrieve a list of all collections from the Milvus vector database.\n",
    "\n",
    "    Makes a GET request to the ingestor API endpoint to fetch all collection names\n",
    "    from the specified Milvus server.\n",
    "\n",
    "    Returns:\n",
    "        Response from the API endpoint containing the list of collections,\n",
    "        or prints error message if request fails.\n",
    "    \"\"\"\n",
    "    url = f\"{INGESTOR_BASE_URL}/v1/collections\"\n",
    "    params = {\"vdb_endpoint\": \"http://milvus:19530\"}\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(url, params=params) as response:\n",
    "                await print_raw_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "await fetch_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.2.3 Delete collections Endpoint\n",
    "\n",
    "**Purpose:**\n",
    "This endpoint deletes list of provided collection names available on the specified vector database server.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "async def delete_collections(collection_names: list[str] = \"\") -> None:\n",
    "    \"\"\"Delete specified collections from the Milvus vector database.\n",
    "\n",
    "    Makes a DELETE request to the ingestor API endpoint to remove the specified\n",
    "    collections from the Milvus server.\n",
    "\n",
    "    Args:\n",
    "        collection_names (List[str]): List of collection names to delete.\n",
    "            Defaults to empty string.\n",
    "\n",
    "    Returns:\n",
    "        None. Prints response from API or error message if request fails.\n",
    "\n",
    "    Example:\n",
    "        await delete_collections(collection_names=[\"collection1\", \"collection2\"])\n",
    "    \"\"\"\n",
    "    url = f\"{INGESTOR_BASE_URL}/v1/collections\"\n",
    "    params = {\"vdb_endpoint\": \"http://milvus:19530\"}\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.delete(\n",
    "                url, params=params, json=collection_names\n",
    "            ) as response:\n",
    "                await print_raw_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# Delete the collection from the previous section\n",
    "print(\"\\nDeleting collection 'multimodal_data1'...\")\n",
    "await delete_collections(collection_names=[\"multimodal_data1\"])\n",
    "\n",
    "# Fetch collections\n",
    "print(\"\\nFetching remaining collections:\")\n",
    "print(\"-\" * 30)\n",
    "await fetch_collections()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Ingestion API Usage\n",
    "\n",
    "##### 2.2.3.1 Upload Document Endpoint\n",
    "\n",
    "**Purpose:**\n",
    "This endpoint uploads new documents to the vector store. \n",
    "1. You can specify the collection name where documents should be stored.\n",
    "\n",
    "2. The collection must exist in the vector database before uploading documents.\n",
    "\n",
    "3. Documents must not already exist in the collection. To update existing documents, use `session.patch(...)` instead of `session.post(...)`\n",
    "\n",
    "4. Multiple files can be uploaded in a single request for efficiency\n",
    "\n",
    "**Configuration Options:**\n",
    "\n",
    "You can customize the document processing with these parameters:\n",
    "\n",
    "- `extraction_options`: Control what content is extracted (text, tables, charts)\n",
    "\n",
    "- `split_options`: Define how documents are chunked (size, overlap)\n",
    "\n",
    "- Custom metadata: Add additional information to your documents\n",
    "\n",
    "**We'll fetch the documents to verify ingestion, and then delete the document.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory containing multimodal documents to upload\n",
    "DATA_DIR = \"../../data/multimodal\"\n",
    "\n",
    "\n",
    "async def upload_documents(collection_name: str = \"\") -> None:\n",
    "    \"\"\"\n",
    "    Uploads documents from DATA_DIR to the specified collection in the vector store.\n",
    "\n",
    "    This function:\n",
    "    1. Reads all files from DATA_DIR\n",
    "    2. Configures extraction and chunking options\n",
    "    3. Uploads documents via POST request to the documents endpoint\n",
    "\n",
    "    Args:\n",
    "        collection_name (str): Name of the collection to upload documents to.\n",
    "                             Collection must exist before uploading.\n",
    "\n",
    "    Extraction options:\n",
    "        - Extracts text, tables and charts by default\n",
    "        - Uses pdfium for extraction\n",
    "        - Processes at page level granularity\n",
    "\n",
    "    Chunking options:\n",
    "        - chunk_size: 1024 tokens\n",
    "        - chunk_overlap: 150 tokens\n",
    "\n",
    "    \"\"\"\n",
    "    # Get list of files from DATA_DIR\n",
    "    files = [\n",
    "        os.path.join(DATA_DIR, f)\n",
    "        for f in os.listdir(DATA_DIR)\n",
    "        if os.path.isfile(os.path.join(DATA_DIR, f))\n",
    "    ]\n",
    "\n",
    "    # Configure upload parameters\n",
    "    # Configure document processing parameters\n",
    "    data = {\n",
    "        # Milvus vector database endpoint\n",
    "        \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "        # Target collection name for document storage\n",
    "        \"collection_name\": collection_name,\n",
    "        # Document extraction configuration\n",
    "        \"extraction_options\": {\n",
    "            \"extract_text\": True,  # Extract text content\n",
    "            \"extract_tables\": True,  # Extract tabular data\n",
    "            \"extract_charts\": True,  # Extract charts/figures\n",
    "            \"extract_images\": False,  # Skip image extraction\n",
    "            \"extract_method\": \"pdfium\",  # Use pdfium PDF parser\n",
    "            \"text_depth\": \"page\",  # Process at page granularity\n",
    "        },\n",
    "        # Text chunking configuration\n",
    "        \"split_options\": {\n",
    "            \"chunk_size\": 1024,  # Size of each text chunk in tokens\n",
    "            \"chunk_overlap\": 150,  # Overlap between chunks in tokens\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Prepare multipart form data with files and config\n",
    "    form_data = aiohttp.FormData()\n",
    "    for file_path in files:\n",
    "        form_data.add_field(\n",
    "            \"documents\",\n",
    "            open(file_path, \"rb\"),\n",
    "            filename=os.path.basename(file_path),\n",
    "            content_type=\"application/pdf\",\n",
    "        )\n",
    "    form_data.add_field(\"data\", json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    # Upload documents\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\n",
    "                f\"{INGESTOR_BASE_URL}/v1/documents\", data=form_data\n",
    "            ) as response:  # Replace with session.patch for reingesting\n",
    "                await print_raw_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "# Upload documents to the multimodal_data collection\n",
    "await upload_documents(collection_name=\"multimodal_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2.3.2 Delete Document Endpoint\n",
    "\n",
    "**Purpose:**\n",
    "This endpoint removes specific documents from the vector store.\n",
    "\n",
    "To demonstrate the functionality of this endpoint, we'll perform a complete document management workflow:\n",
    "\n",
    "| Step | Description | Endpoint |\n",
    "|------|-------------|----------|\n",
    "| 1. Create | Generate a sample text document | N/A |\n",
    "| 2. Upload | Add the document to the vector store | `POST /v1/documents` |\n",
    "| 3. Verify | Check that the document was ingested | `GET /v1/documents` |\n",
    "| 4. Delete | Remove the document from the vector store | `DELETE /v1/documents` |\n",
    "\n",
    "This workflow demonstrates the full lifecycle of document management in the RAG system, allowing you to update your knowledge base as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "# Step 1. Create a sample text document\n",
    "sample_text = \"\"\"This is a sample text document.\n",
    "It contains multiple lines of text.\n",
    "This will be uploaded to the vector store for retrieval.\"\"\"\n",
    "\n",
    "# Create temporary text file\n",
    "with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".txt\", delete=False) as temp_file:\n",
    "    temp_file.write(sample_text)\n",
    "    temp_file_path = temp_file.name\n",
    "\n",
    "try:\n",
    "    data = {\n",
    "        \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "        \"collection_name\": \"multimodal_data\",\n",
    "        \"extraction_options\": {\n",
    "            \"extract_text\": True,\n",
    "            \"extract_tables\": False,\n",
    "            \"extract_charts\": False,\n",
    "            \"extract_images\": False,\n",
    "            \"extract_method\": \"pdfium\",\n",
    "            \"text_depth\": \"page\",\n",
    "        },\n",
    "        # Text chunking configuration\n",
    "        \"split_options\": {\"chunk_size\": 1024, \"chunk_overlap\": 150},\n",
    "    }\n",
    "\n",
    "    # Step 2. Upload file\n",
    "    form_data = aiohttp.FormData()\n",
    "    form_data.add_field(\n",
    "        \"documents\",\n",
    "        open(temp_file_path, \"rb\"),\n",
    "        filename=\"sample_document.txt\",\n",
    "        content_type=\"text/plain\",\n",
    "    )\n",
    "    form_data.add_field(\"data\", json.dumps(data), content_type=\"application/json\")\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\n",
    "                f\"{INGESTOR_BASE_URL}/v1/documents\", data=form_data\n",
    "            ) as response:\n",
    "                await print_raw_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "finally:\n",
    "    # Clean up the temporary file\n",
    "    os.unlink(temp_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now fetch the documents to verify ingestion, and then delete the document.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Fetch documents to verify ingestion\n",
    "async def fetch_documents(collection_name: str = \"\"):\n",
    "    url = f\"{INGESTOR_BASE_URL}/v1/documents\"\n",
    "    params = {\"collection_name\": collection_name, \"vdb_endpoint\": \"http://milvus:19530\"}\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.get(url, params=params) as response:\n",
    "                await print_raw_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "await fetch_documents(collection_name=\"multimodal_data\")\n",
    "\n",
    "\n",
    "# Step 4: Delete the test document\n",
    "async def delete_documents(collection_name: str = \"\", file_names: list[str] = []):\n",
    "    url = f\"{INGESTOR_BASE_URL}/v1/documents\"\n",
    "    params = {\"collection_name\": collection_name, \"vdb_endpoint\": \"http://milvus:19530\"}\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.delete(url, params=params, json=file_names) as response:\n",
    "                await print_raw_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "await delete_documents(\n",
    "    collection_name=\"multimodal_data\", file_names=[\"sample_document.txt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Interacting with the Chatbot\n",
    "\n",
    "Now that all services are deployed, we can interact directly with the RAG chatbot to see how it handles queries with and without using its knowledge base. This section will demonstrate how to programmatically interact with the system, evaluate its responses, and understand the impact of retrieval-augmented generation on the quality of answers.\n",
    "\n",
    "1. **Through the Playground UI** - A visual interface for testing (see [section 1](#1-spin-up-the-blueprint))\n",
    "\n",
    "2. **Through the API** - Programmatic access for integration into applications\n",
    "\n",
    "In this section, we'll focus on API interactions to demonstrate how to:\n",
    "\n",
    "| Interaction Type | Endpoint | Purpose |\n",
    "|------------------|----------|---------|\n",
    "| **Direct LLM queries** | `/v1/chat/completions` with `use_knowledge_base: false` | Use the LLM without RAG |\n",
    "| **RAG-enhanced queries** | `/v1/chat/completions` with `use_knowledge_base: true` | Enhance responses with document knowledge |\n",
    "| **Search** | `/v1/search` | Retrieve documents based on a query |\n",
    "| **Reranking** | `/v1/rerank` | Improve search results with reranking |\n",
    "\n",
    "We'll start with basic LLM queries (no knowledge base) and then show how to leverage the RAG capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Setting Up API Interaction\n",
    "First, let's create a utility function to handle streaming responses from the RAG server. This is particularly important because the LLM generates text token by token, and the API can stream these tokens as they're generated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Streaming Response Handler\n",
    "This function processes the server's streaming response format, concatenating tokens to form the complete response:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def generate_answer(payload):\n",
    "    \"\"\"\n",
    "    Asynchronously generates an answer from the RAG server by sending a POST request with the given payload.\n",
    "\n",
    "    This function handles both streaming and non-streaming responses from the server.\n",
    "    For streaming responses (text/event-stream), it concatenates the content from multiple chunks.\n",
    "    For regular JSON responses, it extracts the content directly from the response.\n",
    "\n",
    "    Args:\n",
    "        payload (dict): The request payload containing messages and other parameters for the RAG server\n",
    "\n",
    "    Returns:\n",
    "        None: Prints the generated content to stdout\n",
    "\n",
    "    The function expects the response to be in one of two formats:\n",
    "    1. Streaming response with Server-Sent Events (SSE)\n",
    "    2. Regular JSON response with a choices->message->content structure\n",
    "    \"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(url=url, json=payload) as response:\n",
    "            # Check if we're getting a streaming response\n",
    "            content_type = response.headers.get(\"Content-Type\", \"\")\n",
    "\n",
    "            if \"text/event-stream\" in content_type:\n",
    "                # Handle streaming response\n",
    "                response_text = await response.text()\n",
    "                concatenated_content = \"\"\n",
    "\n",
    "                for line in response_text.split(\"\\n\"):\n",
    "                    if line.startswith(\"data: \"):\n",
    "                        json_str = line[len(\"data: \") :]\n",
    "                        if json_str.strip() == \"[DONE]\":\n",
    "                            continue\n",
    "                        try:\n",
    "                            json_obj = json.loads(json_str)\n",
    "                            content = (\n",
    "                                json_obj.get(\"choices\", [{}])[0]\n",
    "                                .get(\"delta\", {})\n",
    "                                .get(\"content\", \"\")\n",
    "                            )\n",
    "                            concatenated_content += content\n",
    "                        except json.JSONDecodeError:\n",
    "                            continue\n",
    "\n",
    "                print(concatenated_content)\n",
    "            else:\n",
    "                # Handle regular JSON response\n",
    "                response_json = await response.json()\n",
    "                if \"error\" in response_json:\n",
    "                    print(f\"Error: {response_json['error']}\")\n",
    "                    return\n",
    "\n",
    "                content = (\n",
    "                    response_json.get(\"choices\", [{}])[0]\n",
    "                    .get(\"message\", {})\n",
    "                    .get(\"content\", \"\")\n",
    "                )\n",
    "                print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Test Endpoint Health\n",
    "Verify the health of the RAG server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aiohttp\n",
    "\n",
    "# Testing RAG Server health\n",
    "print(\"\\nStep 1: Testing RAG server health endpoint\")\n",
    "async with aiohttp.ClientSession() as session:\n",
    "    async with session.get(f\"{RAG_BASE_URL}/v1/health\") as response:\n",
    "        await print_raw_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Preparing Test Documents\n",
    "To demonstrate the difference between standard LLM responses and RAG-enhanced responses, we'll ingest a document containing information about FIFA World Cup winners. This will allow us to compare how the system responds to queries with and without access to this knowledge.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Downloading the Sample Document\n",
    "First, let's download a sample document about FIFA World Cup winners:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O fifa_world_cup_winners.pdf https://s3-ap-south-1.amazonaws.com/adda247jobs-wp-assets-adda247/jobs/wp-content/uploads/sites/2/2022/12/20155949/FIFA-World-Cup-Winners-List-From-1930-to-2022.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Ingest Document into Knowledge Base\n",
    "Now we need to process this document and add it to our vector database:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name for the compose service\n",
    "IPADDRESS = \"ingestor-server\"\n",
    "\n",
    "# Port number for the service\n",
    "rag_server_port = \"8082\"\n",
    "\n",
    "# Base URL constructed from IP and port for making API requests\n",
    "INGESTOR_BASE_URL = f\"http://{IPADDRESS}:{rag_server_port}\"\n",
    "\n",
    "\n",
    "# Upload FIFA World Cup Winners PDF\n",
    "async def upload_fifa_document(collection_name: str = \"\") -> None:\n",
    "    fifa_pdf_path = \"fifa_world_cup_winners.pdf\"\n",
    "\n",
    "    data = {\n",
    "        \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "        \"collection_name\": collection_name,\n",
    "        \"extraction_options\": {\n",
    "            \"extract_text\": True,\n",
    "            \"extract_tables\": True,\n",
    "            \"extract_charts\": True,\n",
    "            \"extract_images\": False,\n",
    "            \"extract_method\": \"pdfium\",\n",
    "            \"text_depth\": \"page\",\n",
    "        },\n",
    "        \"split_options\": {\"chunk_size\": 1024, \"chunk_overlap\": 150},\n",
    "    }\n",
    "\n",
    "    form_data = aiohttp.FormData()\n",
    "    form_data.add_field(\n",
    "        \"documents\",\n",
    "        open(fifa_pdf_path, \"rb\"),\n",
    "        filename=\"fifa_world_cup_winners.pdf\",\n",
    "        content_type=\"application/pdf\",\n",
    "    )\n",
    "    form_data.add_field(\"data\", json.dumps(data))\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(\n",
    "                f\"{INGESTOR_BASE_URL}/v1/documents\", data=form_data\n",
    "            ) as response:\n",
    "                await print_raw_response(response)\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "await upload_fifa_document(collection_name=\"multimodal_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Comparing LLM vs. RAG Responses\n",
    "Now let's compare how the system responds to the same query with and without leveraging the knowledge base.\n",
    "\n",
    "#### 3.3.1 Query Without Knowledge Base\n",
    "First, let's see how the model responds using only its pre-trained knowledge:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name for the compose service\n",
    "IPADDRESS = \"rag-server\"\n",
    "\n",
    "# Port number for the service\n",
    "rag_server_port = \"8081\"\n",
    "\n",
    "# Base URL constructed from IP and port for making API requests\n",
    "RAG_BASE_URL = f\"http://{IPADDRESS}:{rag_server_port}\"\n",
    "\n",
    "url = f\"{RAG_BASE_URL}/v1/chat/completions\"\n",
    "payload = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Who won the last FIFA World Cup?\"}],\n",
    "    \"use_knowledge_base\": False,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.7,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"reranker_top_k\": 10,\n",
    "    \"vdb_top_k\": 100,\n",
    "    \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "    \"collection_name\": \"multimodal_data\",\n",
    "    \"enable_query_rewriting\": False,\n",
    "    \"enable_reranker\": True,\n",
    "    \"enable_guardrails\": False,\n",
    "    \"enable_citations\": True,\n",
    "    \"model\": \"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n",
    "    \"llm_endpoint\": \"nim-llm:8000\",\n",
    "    \"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "    \"embedding_endpoint\": \"nemoretriever-embedding-ms:8000\",\n",
    "    \"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "    \"reranker_endpoint\": \"nemoretriever-ranking-ms:8000\",\n",
    "    \"stop\": [],\n",
    "}\n",
    "\n",
    "await generate_answer(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's happening:**\n",
    "\n",
    "> - The request sets use_knowledge_base: false\n",
    ">\n",
    "> - The model relies solely on its pre-trained knowledge\n",
    "> - No document retrieval or context augmentation occurs \n",
    "> - The response is based on what the model \"knows\" from training\n",
    "> - For recent or specialized information, this may lead to outdated or incorrect answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.2 Query With Knowledge Base (RAG)\n",
    "Now, let's run the same query but enable the knowledge base:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name for the compose service\n",
    "IPADDRESS = \"rag-server\"\n",
    "\n",
    "# Port number for the service\n",
    "rag_server_port = \"8081\"\n",
    "\n",
    "# Base URL constructed from IP and port for making API requests\n",
    "RAG_BASE_URL = f\"http://{IPADDRESS}:{rag_server_port}\"\n",
    "\n",
    "url = f\"{RAG_BASE_URL}/v1/chat/completions\"\n",
    "payload = {\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Who won the last FIFA World Cup?\"}],\n",
    "    \"use_knowledge_base\": True,\n",
    "    \"temperature\": 0.2,\n",
    "    \"top_p\": 0.7,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"reranker_top_k\": 10,\n",
    "    \"vdb_top_k\": 100,\n",
    "    \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "    \"collection_name\": \"multimodal_data\",\n",
    "    \"enable_query_rewriting\": False,\n",
    "    \"enable_reranker\": True,\n",
    "    \"enable_guardrails\": False,\n",
    "    \"enable_citations\": True,\n",
    "    \"model\": \"nvidia/llama-3.3-nemotron-super-49b-v1.5\",\n",
    "    \"llm_endpoint\": \"nim-llm:8000\",\n",
    "    \"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "    \"embedding_endpoint\": \"nemoretriever-embedding-ms:8000\",\n",
    "    \"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "    \"reranker_endpoint\": \"nemoretriever-ranking-ms:8000\",\n",
    "    \"stop\": [],\n",
    "}\n",
    "\n",
    "await generate_answer(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's happening:**\n",
    "\n",
    "> - The request sets use_knowledge_base: true\n",
    ">\n",
    "> - The system converts the query into an embedding\n",
    "> - It searches the vector database for semantically similar document chunks\n",
    "> - The retrieved chunks are added as context to the LLM prompt\n",
    "> - The model generates a response based on this augmented context\n",
    "> - The response should contain specific information from our ingested document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Understanding Document Retrieval and Reranking \n",
    "### 4.1 Utility function to print search results\n",
    "First, let's create a utility function to better visualize search results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_search_results(response):\n",
    "    \"\"\"\n",
    "    Nicely formats and prints search results from the RAG system.\n",
    "    Also renders base64 encoded images when present.\n",
    "\n",
    "    Args:\n",
    "        response (dict): The response from the search API\n",
    "\n",
    "    Returns:\n",
    "        None: Prints formatted results to the console and displays images when applicable\n",
    "    \"\"\"\n",
    "    if \"results\" not in response or \"total_results\" not in response:\n",
    "        print(\"Invalid response format or no results found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n=== SEARCH RESULTS ({response['total_results']} total) ===\\n\")\n",
    "\n",
    "    for i, result in enumerate(response[\"results\"], 1):\n",
    "        print(f\"Result #{i}\")\n",
    "        print(f\"Document: {result.get('document_name', 'Unknown')}\")\n",
    "        print(f\"Score: {result.get('score')}\")\n",
    "\n",
    "        # Handle different document types\n",
    "        document_type = result.get(\"document_type\", \"text\")\n",
    "        print(f\"Type: {document_type}\")\n",
    "\n",
    "        print(\"\\nContent:\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "        content = result.get(\"content\", \"\")\n",
    "\n",
    "        # Check if content looks like base64 image data\n",
    "        # Base64 images typically start with specific patterns\n",
    "        is_likely_image = False\n",
    "        if content and isinstance(content, str):\n",
    "            # Common base64 image prefixes to check\n",
    "            image_prefixes = [\"iVBOR\", \"/9j/\", \"R0lGOD\", \"PD94\", \"PHN2\"]\n",
    "            is_likely_image = any(\n",
    "                content.startswith(prefix) for prefix in image_prefixes\n",
    "            )\n",
    "\n",
    "        # For chart/image type documents or content that looks like base64\n",
    "        if document_type in [\"chart\", \"image\"] or is_likely_image:\n",
    "            try:\n",
    "                import base64\n",
    "                import io\n",
    "\n",
    "                from IPython.display import display\n",
    "                from PIL import Image\n",
    "\n",
    "                # Get base64 string and decode\n",
    "                img_data = base64.b64decode(content)\n",
    "\n",
    "                # Convert to PIL Image\n",
    "                img = Image.open(io.BytesIO(img_data))\n",
    "\n",
    "                # Display the image\n",
    "                display(img)\n",
    "\n",
    "                # Print metadata description if available\n",
    "                if \"metadata\" in result and \"description\" in result[\"metadata\"]:\n",
    "                    print(f\"\\nDescription: {result['metadata']['description']}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error rendering image: {str(e)}\")\n",
    "                print(\"Raw content (base64 encoded, truncated):\")\n",
    "                if len(content) > 100:\n",
    "                    content = content[:100] + \"... [base64 content truncated]\"\n",
    "                print(content)\n",
    "        else:\n",
    "            # For text documents, print the content with optional truncation\n",
    "            if len(content) > 500:\n",
    "                content = content[:500] + \"... [content truncated]\"\n",
    "            print(content)\n",
    "\n",
    "        print(\"-\" * 80)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Basic Search Without Reranking\n",
    "Let's perform a basic search query without reranking:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search without ranking\n",
    "url = f\"{RAG_BASE_URL}/v1/search\"\n",
    "payload = {\n",
    "    \"query\": \"\",\n",
    "    \"reranker_top_k\": 3,\n",
    "    \"vdb_top_k\": 100,\n",
    "    \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "    \"collection_name\": \"multimodal_data\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the rationale for Clear Print Guidelines\"}\n",
    "    ],\n",
    "    \"enable_query_rewriting\": False,\n",
    "    \"enable_reranker\": False,\n",
    "    \"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "    \"embedding_endpoint\": \"nemoretriever-embedding-ms:8000\",\n",
    "    \"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "    \"reranker_endpoint\": \"nemoretriever-ranking-ms:8000\",\n",
    "}\n",
    "\n",
    "\n",
    "async def document_search(payload):\n",
    "    \"\"\"\n",
    "    Performs a search against the RAG system and prints formatted results.\n",
    "\n",
    "    Args:\n",
    "        payload (dict): The search query payload\n",
    "\n",
    "    Returns:\n",
    "        dict: The raw response from the API\n",
    "    \"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url=url, json=payload) as response:\n",
    "                response_json = await response.json()\n",
    "                print_search_results(response_json)  # Format and print the results\n",
    "                return None  # Still return the response but it won't be printed\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "await document_search(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Enhanced Search With Reranking\n",
    "Now let's see how reranking improves search results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with ranking\n",
    "url = f\"{RAG_BASE_URL}/v1/search\"\n",
    "payload = {\n",
    "    \"query\": \"\",\n",
    "    \"reranker_top_k\": 3,\n",
    "    \"vdb_top_k\": 100,\n",
    "    \"vdb_endpoint\": \"http://milvus:19530\",\n",
    "    \"collection_name\": \"multimodal_data\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"user\", \"content\": \"What is the rationale for Clear Print Guidelines\"}\n",
    "    ],\n",
    "    \"enable_query_rewriting\": False,\n",
    "    \"enable_reranker\": True,\n",
    "    \"embedding_model\": \"nvidia/llama-3.2-nv-embedqa-1b-v2\",\n",
    "    \"embedding_endpoint\": \"nemoretriever-embedding-ms:8000\",\n",
    "    \"reranker_model\": \"nvidia/llama-3.2-nv-rerankqa-1b-v2\",\n",
    "    \"reranker_endpoint\": \"nemoretriever-ranking-ms:8000\",\n",
    "}\n",
    "\n",
    "\n",
    "async def document_search(payload):\n",
    "    \"\"\"\n",
    "    Performs a search against the RAG system and prints formatted results.\n",
    "\n",
    "    Args:\n",
    "        payload (dict): The search query payload\n",
    "\n",
    "    Returns:\n",
    "        dict: The raw response from the API\n",
    "    \"\"\"\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        try:\n",
    "            async with session.post(url=url, json=payload) as response:\n",
    "                response_json = await response.json()\n",
    "                print_search_results(response_json)  # Format and print the results\n",
    "                return None  # Still return the response but it won't be printed\n",
    "        except aiohttp.ClientError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "await document_search(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's happening:**\n",
    "\n",
    "> - Initial retrieval is performed using vector similarity (same as basic search)\n",
    ">\n",
    "> - The reranker model then examines each retrieved document more carefully\n",
    "> - It scores the relevance of each document to the specific query\n",
    "> - Results are reordered based on these relevance scores\n",
    "> - The most relevant documents appear at the top\n",
    "Notice how the scores change and the ordering of results may differ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "Now that you have deployed and tested the RAG system, here are some suggested next steps:\n",
    "\n",
    " - Upload your own documents: Try ingesting different document types (PDFs, text files, etc.)\n",
    "\n",
    " - Experiment with parameters: Adjust temperature, top_k values, and other settings\n",
    "\n",
    " - Compare quality: Set up side-by-side comparisons of RAG vs. non-RAG responses\n",
    "\n",
    " - Integrate with applications: Use the APIs to build your own applications on top of this system\n",
    " \n",
    " - Custom prompt engineering: Experiment with different prompt formats to improve response quality\n",
    "\n",
    "The system you've deployed provides a powerful foundation for building AI-powered applications that can leverage both the knowledge in your documents and the capabilities of large language models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
